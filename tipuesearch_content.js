var tipuesearch = {"pages":[{"title":"Quantifying the Expected Value of a Second","text":"Introduction Basketball analytics have come a long way in recent years. A lot of work has gone into \"efficient basketball\" (dubbed Morey-ball by some) on how to optimize shot distribution. This has led to teams sacrificing the midrange shots for a layup or a three-pointer. In this post, I wish to do a deep dive into a lesser explored topic: the shot clock and how it can be used as a decision making tool to play optimal basketball. For instance, a semi-contested three with 20 seconds left on the clock is probably not an ideal shot, but what if there is only 5 seconds left on the clock? Is there a way quantify shot quality as it relates to the shot clock? Background Knowledge Shot Clock In the 1954-1955 NBA season, the NBA introduced the 24-second shot clock to speed the game up. The shot clock resets when there is a clear change of possession by either a defensive rebound, a turnover, or a made basket. If there is a kick ball violation, foul, or offensive rebound 1 , the shot clock resets to 14 seconds. Teams have at most 8 seconds to get the ball across the half court line, so there is always at least 16 seconds in the 'half-court' setup. Points Per Possession Every offensive possession has a non-negative expected value, ranging from 0 to 3 2 . A team can miss or turn the ball over in which case they score 0 points this possession, or hit a three-pointer in which case they score 3 points. Teams will have different expected points per possession (PPP), and certain players are more efficient scorers than others, but in large part the PPP of a team ranges from about 1-1.15. Since teams typically have around 100 possessions per game, a .1 difference in PPP is very large and over the course of the game ends up being 10 points, the difference between a lottery team and a championship contender. Thesis Assume the Lakers are expected to score 1.2 points every possession and we denote this expression as $E[PPP|\\text{24 seconds}]=1.2$. It is important to understand that this 1.2 number is with a full 24 second shot clock and should fall as the shot clock declines. Taking the extreme, if there was only 1 second left on the shot clock, the $E[PPP|\\text{1 second}]$ would be strictly less than 1.2 (probably more like .5) 3 . I wish to quantify the difference in $E[PPP]$ as it relates to the shot clock, so every second on the 24 second shot clock will have a different $E[PPP]$. My working hypothesis is that there is some sort of logarithmic distribution from 0-24 seconds. The $E[PPP]$ will be low in the beginning and then gradually rise and top off at 24 seconds. Below, I have created a model of what I believe the trade-off should look like 4 . With 1 second left on the shot clock, anything thrown to the rim is better than an alternative of a turnover so I think $E[PPP|\\text{1 second}]$ is close to the expected value of a contested fade-away. At 24 seconds on the shot clock, we have the full 1.2 number, but the relationship should not be a linear one. NBA teams typically do not need the full 24 seconds to shoot and can probably generate a decent shot with half the time. More time just gives them the optionality to turn down good shots for great shots. I use an inverse exponential model which tails off later in the shot clock. Data I was unable to get recent up to date NBA shot clock data as it seems the NBA back-end does not show shot clock data, but it still has a bunch of other (harder to collect) stats like closest defender distance and location on court. If anyone knows how to collect this data please get in touch, I would love to use up-to-date data 5 . However, I found historical shot-log data 6 which included shot clock for the 2014-15 season. This data includes over 100,000 shots with at least 1000 data points for each second of the shot clock so it is decently robust. Although NBA teams have gotten more efficient over the past few years, I believe the overall relationship between PPP and shot clock should remain stable. After doing some data cleaning like removing end of quarter situations when the shot clock is turned off to avoid rushed heaves, I group the data by second and look at the $E[PPP]$ at each of these seconds to create the plot below. I shall refer to this plot as the PPP trade-off curve. We see a similar shape to my predicted curve, where at 1 second, the PPP is below .6 and at 24 seconds the PPP is around 1.2. An interesting observation is that the PPP peaks at around 22 seconds as opposed to 24 seconds, and I suspect this has to do with offensive rebounds and immediate put backs. The shots with 23 and 24 seconds left on the shot clock are offensive rebounds and thus contested close to the basket shots, whereas the shots with 21 and 22 seconds left on the shot clock could be great transition looks as it takes a few seconds to get down the court 7 . Also, it is interesting that there is a slight increase in efficiency around 12 and 13 seconds left on shot clock as opposed to 14 and 15, which could be due to the fact that shot clocks reset to 14 seconds after fouls. This then enables a team to run a scripted out of bounds play which could lead to a cleaner look and more efficient shot. Game Adjustments I want to clarify that these numbers are in the context of generating good looks. Just because people score around 1.1 PPP when there is 24 seconds left on the shot clock does not mean launching full court shots is a winning basketball strategy. There are a lot of extensions that can be built upon this analysis, but it hints that NBA teams should always be pushing the pace to give them more opportunities to generate a clean look. Often times we see point guards just casually bring the ball up the court, but they should probably be sprinting as every second is valuable. From a more mathematical perspective the PPP trade-off curve can dictate optimal basketball strategy, like when to swap a good shot for a great shot, which play to run, and even which players should be on the court. When to shoot? In an optimal world, teams are able to calculate the expected value of any shot using a few important features such as: Who is shooting the ball? Who is the closest defender? What is the distance of the closest defender? What is the shot location? How much time is left on the shot clock? This information can then be extrapolated and used as a decision making tool. If the player believes that his current shot is highest expected value his team is going to get this possession then he should shoot it; else he should turn the shot down. This is a very hard decision, because players do not know the expected value in the future; at best the player holding the ball can pass to a teammate and calculate his teammate's expected value of a shot using the same criteria above. Besides the one pass ahead calculation (which will have a lot of variance), most of the time a player just would not know the future value of this possession, so it brings a game theory question of what to do with the ball. A player can rely on the PPP trade-off curve to aid in his decision making process. On a broad level, if a shot creates more expected value than the $E[PPP]$ at the given shot clock time, a player should probably shoot the ball. This provides his team with higher than average PPP which by definition is a winning strategy in the long run 8 . Using the same graph as above, I shaded the green area to represent optimal area to shoot and shall refer to this area as the green zone. Most of the time the optimal strategy is to take the first shot that is in the green area, as the expected value of the possession decreases the longer a team holds the ball. I say most, because as mentioned previously there are occasions where the extra pass leads to an increase in $E[PPP]$ and players need to weigh this trade off carefully. Coaches Play Calling Coaches can also follow the trade-off curve when calling plays. The Warriors are actually a prime example of a team successfully implementing the PPP trade-off curve, even though they never formalized this concept. For the majority of the shot clock, the Warriors will run a variety of screens to free up Curry or Thompson to get them a comfortable shot. If the plays are successful in creating a clean look, the shot would instantly be in the green zone (regardless of shot clock) as both are prolific 40%+ shooters from deep. A classic example is this elevator screen where 3 other Warriors players work in unison to get Curry an open look. Lets take a closer look at the elevator screen referenced above. Notice the shot clock is at 7 seconds during the initial pin-down and then Curry catches it with about 4 seconds left. A wide open Curry three is worth at least 1.2 points, but what if the Jazz switched and blew this play up? Then Klay would be isoed against Gordon Hayward 30 feet from the hoop with 3 seconds left on the shot clock, and I estimate this to probably be around .6 points. Using the PPP trade-off curve as a guideline, one can judge if the elevator screen was a good play call with 7 seconds left on the shot clock. Let us assume that the elevator screen play takes 4 seconds to run. We can then formalize the question: if the expected value of the elevator screen play plus the expected value of the 3 seconds after that are greater than the $E[PPP|\\text{7 seconds}]$, the elevator screen is a good play to call. Let us break the outcome of the elevator screen play down into two states, the state that Curry gets a clean look and the state that Curry does not so Thompson keeps the ball. This would lead us to a formula to calculate $E[\\text{Elevator Screen}|7 seconds]$. \\begin{align} E[\\text{Elevator Screen}|7 seconds] &= P(ES)* E[\\text{Curry 3PA}] + (1-P(ES))* E[\\text{Iso}|\\text{3 Seconds}] \\\\ \\text{where} & \\\\ E[\\text{Elevator Screen}|7 seconds] &= \\text{Expected Value of the Elevator Screen given 7 seconds left} \\\\ P(ES) &= \\text{Probability elevator screen is run successfully} \\\\ E[\\text{Curry 3PA}] &= \\text{Expected value of Curry getting a relatively clean look from three} \\\\ E[\\text{Iso}|\\text{3 Seconds}] &= \\text{Expected value of Klay isoing given 3 seconds left} \\\\ \\end{align} If $E[\\text{Elevator Screen}|7 seconds]=E[PPP|\\text{7 seconds}]$, then the play was an average play, and the delta between these variables will determine how good or bad of a play it is. Maintaining the prior assumptions that a Curry three is worth 1.2 points, a Klay iso (with 3 seconds left) is worth .6 points, and $E[PPP|\\text{7 seconds}]$ is .9 points, I will vary $P(ES)$ values to determine the possible range of $E[\\text{Elevator Screen}|7 seconds]$. Looking at the graph below, I plot two lines $E[\\text{Elevator Screen}|7 seconds]$ and $E[PPP|\\text{7 seconds}]$ to see at which point the trade off occurs. So to answer the original question, if the Warriors can get the $P(ES)$ above .5, then the play is good, as they will be in the green zone. If they cannot then they should run a different play with 7 seconds left. Decision Making Detailed The above example is a scenario in which the primary option was open, but it is not always that simple. Sometimes the defense rotates to recover and it is during these instances the PPP trade-off curve can be used effectively. Here we have a couple of screens that do not lead anywhere until a Thompson Green pick and roll with 10 seconds left in the shot clock. Draymond Green catches the ball wide open with six seconds left on the shot clock and the picture below captures this exact moment. Green ends up turning that shot down to pass to Livingston under the hoop and I argue that this was a suboptimal decision with respect to the PPP trade-off curve. If he shoots the ball there, it roughly translates to 1.16 points given that he was shooting 38.8% from three that year and he is wide open. This 1.1 number is much higher than the typical points expected with 6 seconds left in the shot clock, so he probably should have shot it. The counter argument is if he thinks Livingston under the hoop versus Iman Shumpert is a higher expected value than 1.16 points which would mean Livingston would have to shoot above 58% on his shot, which I do not think is likely. Just pulling up Livingston's shooting splits and his proximity to the hoop which I estimate at around 5 feet 9 , I see that he shot around 45% from that distance, giving an expected value of 0.9 points on that shot. This play by Draymond ends up turning into a positive because Lebron inexplicably doubles, leaving JR Smith to fend off Thompson and Barbosa. JR Smith then compounds Lebron's mistake by leaving Klay Thompson one of the greatest shooters in NBA history open to contest Barbosa, a career average 3-point shooter. Tough Shots Finally, certain players are better at scoring tough shots than others. This skill is not very useful when there is 20 seconds left on the shot clock, but is very useful when there is 5 seconds off the shot clock. Many times plays do not work, counters to plays do not work, and the last resort is to throw it to a player and let him create something out of nothing. Going back to the Warriors, Kevin Durant was notorious for the bailout call, where he would get the ball with less than five seconds left and shoot a contested jump shot efficiently. His ability to hit difficult shots at an elite clip complimented the Curry and Thompson screens and this is why the Warriors had one of the best offensives in the history of the NBA. Final Thoughts Current boxscore basic and advanced stats come up short in evaluating tough shots, because they group the shots together without looking at circumstances such as shot clock. Shooting 45% on tough shots is not efficient when there is plenty on time on the clock, but it is elite if there is only a few seconds left. There should be some metric that quantifies the ability to score while the shot clock is low. If a player holds the ball for a long time or drives to the hoop without creating any shot, he is actually harming the expected points of the possession. However, this player receives no direct negative statistic in the boxscore. Another variation of this is when teams spend a majority of the shot clock to post up a player and the pass never gets thrown, either due to poor positioning by the guy in the post or the person throwing the ball. One of these players should be penalized for the seconds wasted but this does not exist in current stats. The exact counter point is when the defender shuts down a drive. This defender does not receive any accolades for chopping seconds off the shot clock as the play must now reset. Teams can go through a game and rate every possession to see when a shot was taken in the green zone and when a shot was passed up in the green zone. They can then attribute these plays to individual players, to see which players make good decisions and vice versa. Offensive rebounds use to reset to the entire 24 second shot clock, but this was changed for the 2019-20 season. ↩ Technically, there is the ability to get a 4 point play with a three pointer plus a foul or even 5 points with a flagrant foul and a three pointer. ↩ This is inherently due to the quality of the shot when the shot clock winding down being worse. ↩ These values are rounded up, just like on the NBA shot clock so even if there is .1 seconds left on the shot clock it is counted as 1 second left. ↩ An alternative would be to parse the play-by-play data, and code up logic to calculate the shot clock, but I am trying to avoid that path for now. ↩ It is worth noting that the data unfortunately only includes field goals and not free throws or turnovers. ↩ I believe that using the new 2019-2020 season data which resets shot clock to 14 after offensive rebounds will change this shape dramatically with regards to the 23 and 24 seconds shots. ↩ Teams can customize the PPP trade-off curve to add in their own percentages as they may be more or less efficient than league average ↩ The entire paint is 12 feet wide, half of that would be six feet and he looks to be slightly inside the paint ↩","tags":"NBA","url":"/shot-clock.html","loc":"/shot-clock.html"},{"title":"An Analysis into Gold","text":"Simple Gold Fair Value Model A quick and dirty way to price gold is to look at the total amount of money in circulation (M2) and divide it by the size of gold stock. This would quite literally tell how much gold is worth. It is kind of difficult to precisely measure all of the gold stocks in the world, but there are rough estimates we can use. Splitting gold into jewellery, private investment, official holdings, and other, we see around 197.6k tonnes in existence at the end of 2019. In addition, we know that mining adds around 2.5-3k tonnes annually. Given that we are midway through 2020, I will add 1.5 tonnes and assume that we have a total of 199.1k tonnes of gold in existence. Turning this into troy ounces, we get around 6.4 billion troy ounces of gold. As of July 16th 2020, M2 money stock was 18.522 trillion giving gold a fair value price of 2894 per troy ounce and at the current gold price of around 1870 there is roughly a 35% premium. Unfortunately, I cannot get a time series of global gold stock to see compare the price of gold versus this fundamental calculation. However, I will try to estimate this number since the removal of the Bretton Woods agreement where we were taken off the gold standard by looking at the amount of gold produced annually 1 and subtracting it from the 197.6k tonnes at the end of 2019. Using the same formula as above, I calculate the theoretical price of Gold and plot it against the traded price. We can see that gold typically trades at a large discount, but there are periods that it catches up or even surpasses the fundamental price. So why am I so bullish on gold right now? Below is a plot of M2 for the past 5 years. The line grew very steadily with a constant slope until March 2020, when the Fed relaxed lending rules ( I , II , III , and IV ) and M2 took off like a rocket. Now gold is knocking at all time highs (in dollar terms) and I do not see the Fed reversing their policies anytime soon. The tailwinds are in-place for another gold run. A side note is that other central banks like the ECB are also engaging in similar techniques increasing the global money supply. Further more the stimulus checks that the government hands out are were delivered via direct deposit into the checking account for around 90 million Americans. The same will be true for the purposed second round of stimulus checks and both of these stimulus checks are direct increases of money in circulation. Other Factors on Gold Price The above model is a pretty rough estimate which only looks at two factors, money in circulation and gold stock. Some of the other main drivers are below and I will examine a few of these in detail. Gold price moves inversely to real interest rates. Gold is correlated to central bank balance sheet expansion. Like any other asset gold price is related to demand and supply. Gold is a safe haven asset and will rise in periods of global uncertainty. Gold is correlated to other commodities and will rise and fall in tandem. Interest Rate One of the strongest predictive and contemporaneous metrics to gold price is real interest rates. When real rates (interest rate minus inflation) are low or negative, the real return of bonds are unattractive as an asset class and people look for alternatives. Claude Erb and Campbell Harvey look at the relationship between real interest rates and gold price and see a -.82 correlation between the two. We can use the 10-year TIPs (Treasure Inflation-Index Securities) to proxy the real interest rates and see how the relationship has performed in the past few years. From 2003 to present day there is a -.88 correlation and below is a plot of the real rates and gold price. We see that other than a few liquidity events which impacted the price of gold but not real rates, there is a clear relationship inverse relationship between the two. Another way to visualize the strength of this relationship is to just multiply the real rates by -1, and look at the graph of inverse real rates to gold price (also plotted below). Even before COVID-19 we had negative real rates and COVID-19 just accelerated the decline to levels not seen in a few decades. Until there is a cure for COVID-19, I do not see how this trend is going to be reversed and even with a cure, some economists are projecting years until we get back to pre-corona virus levels of economic activity. Central Bank Balance Sheet Aggregating some of the largest central banks in the world (Fed, ECB, BoJ) 2 , I created a total balance sheet of these central banks. Then I take rolling 12 month differences between this aggregated global bank balance sheet and the price of gold. I am able to see that gold typically appreciates in value when the Central Banks expand their balance sheet. The major exception to this case would be in 2008, when gold tanked, but that was more due to a liquidity crisis where people were selling anything to get cash. In 2020, the Fed is taking unprecedented action to inject liquidity into the market by buying corporate bond ETFs. While I expect the expansion of the balance sheet to slow down, there is no indication that the Fed is planning to shrink the balance sheet anytime soon and this indicator is bullish. Central Bank Gold Reserves One of the main drivers of gold demand is central banks. We can see that central banks have been net buyers of gold. There are around 2500 to 3000 tonnes of gold mined annually and central banks are buying around 15-20% of the new gold and adding it to their reserves. Below we can see that cumulative central bank reserves have been increasing by around 1000 tonnes every 2-3 years or the past 12 years. This trend should continue into the foreseeable future and some central banks (Russia and China) have even talked about increasing reserve purchase amounts. ETF Flows A good proxy for gold supply and demand are the ETFs as they are a lot more liquid and accessible for the everyday investor. A lot of these ETFs charge high expense ratios, because they store the physical gold. From gold.org , we can see the historical ETF flows around the world from 2003 till current day. I want to note the strong correlation between gold price on the right hand side and ETF flows. Positive ETF flows typically create positive gold prices and vice versa. Looking at this chart, only one year has there been positive ETF flows and a drop in gold price. The second point I would like to note is the historic amount of ETF inflows 2020 has seen. We are only halfway done the year, and already there is more than 500 tonnes of gold ETF inflows, the most in any year so far. I fully expect more gold inflows and there is a lot of price reflexivity here. As gold price increases, more people get bullish on gold and this creates an feedback loop. It has been 9 years, but gold is finally nearing all time highs in dollar terms. Conculsion Many of the historical gold indicators are flashing buy signs at the same time. I believe that gold is posied for a continued breakout past its all time highs. In the next few posts, I will talk about other some other assets that will rise in tandem with gold and some other investment possibilities. The gold production data was collected from the United States Geological Survey, which has annual statistics on a variety of minerals. ↩ PBoC is the other large central bank, but I was unable to find up to date data on its holdings. ↩","tags":"Investing","url":"/metals-1.html","loc":"/metals-1.html"},{"title":"Dell VMWare split","text":"Dell Technologies, \\$DELL, owns 81% of VMware, \\$VMW, through its \\$67 billion purchase of EMC. Looking at the prices at the close of 2020-07-16, we see that VMWare is worth 58.61B while Dell is worth 43.78B. This means that the portion of stock in VMWare Dell owns is worth 47.5B. According to the market, Dell without VMware is worth around negative 4B. It seems that a spin-off is in play and would be beneficial to both companies. Dell bought VMW in September of 2016 so he needs to wait 5 years till 2021 and there will be a spinoff in a tax-free transaction fashion. For what its worth, the closest competitor to \\$DELL, \\$HPE trades at a similar PE ratio to \\$DELL, has similar margins in the PC space and is worth 12B. In 2013, Michael Dell and Silver Lake Partners bought out Dell's shares in the public market and took Dell Technologies private. 5 years later, Dell Technologies returned to the public makets by buying back shares that tracked the financial performance of VMware. Using historical market caps (from ycharts), I have plotted a time series graph of \\$DELL marketcap without the \\$VMW portion. The exact forumla is below \\begin{align} D_{\\text{val}} &= DELL_{\\text{Market Cap}}-.81*VMW_{\\text{Market Cap}} \\\\ \\text{where} &= \\\\ D_{\\text{val}} &= \\text{Market Cap of Dell Technologies without VMWare} \\\\ DELL_{\\text{Market Cap}} &= \\text{Current Market Cap of Dell Technologies} \\\\ VMW_{\\text{Market Cap}} &= \\text{Current Market Cap of VMWare} \\\\ \\end{align} We can see that the intrinsic value of $D_{\\text{val}}$ started at around -10 billion when it listed and has hovered mainly between -20 billion and -10 billion. It is currentely at an all time high of -4 billion due to \\$VMW spinoff rumors. I believe that the fair value of \\$DELL would be in the ballpark of \\$HPE given that they are competitors and \\$DELL has higher gross profit and profit margin along with a similar PE ratio. Even if $DELL is only worth 10B, thats still a 15B increase on the current -4B price and almost a 35% increase on the current market cap. Potential Trades I think there is the obvious trade which is to long \\$DELL and then there are two hedged plays which may be more lucrative from a risk-reward standpoint. The first hegdged trade would be to long \\$DELL and short \\$HPE, which is betting on both \\$VMW increasing and \\$DELL increasing from the -4 billion valuation. A more involved play would be to short both \\$HPE and \\$VMW and only bet on Dell's -4 billion valuation increasing to something reasonable. For what its worth I am long \\$DELL, because I believe that they will spin-off the VMWare shares and I wish to be long \\$VMW as I think it is a very cheap business with highly profitable margins in the SaaS space. I will write a followups for why I like \\$VWM and track how the proposed trades suggested have done.","tags":"Investing","url":"/dell-vmware.html","loc":"/dell-vmware.html"},{"title":"Ampleforth","text":"In the paper Synthetic Commodity Money , George Selgin describes Bitcoin and other cryptocurrencies as synthetic commodities that address shortcomings traditional stores of value have. Specifically fiat money has mismanagement issues (queue present day Federal Reserve money printer) and commodities have unexpected supply shocks. The issue with Bitcoin is that there is a fixed amount, so deflation is built in. Another cryptocurrency with more elastic supply would have the benefits Bitcoin has as well as fix the deflationary issue. Ampleforth tries to tackle this exact issue. Ampleforth, \\$AMPL, is one of the most interesting coins, because the supply of it changes daily based on the price. When the price is high, the wallet balances increases and when it is low the balance decreases. The main purpose of this rebase is to dampen the volatility of the price, giving it an intrinsic ability to be a stablecoin and price other assets off of Ampleforth. In essence, the larger the market cap, the more stable \\$AMPL price will be. The whitepaper can be found here . How the rebase works works I will outline two specific scenarios. Jordan buys 1 Ample for $1 and the price soon increases to $2. The price-supply equilibrium would 'split' the Amples and Jordan would end up with 2 Amples worth $1 each. The total net worth of Amples is constant, this is literally a stock split. Say after this Jordan's Amples shoot down to $.5. Similarly, after the rebase period, he will end up with 1 Ample worth $1. This is akin to a reverse stock split The exact details are subject to change but currently if the price is above a target threshold the supply expands and if it is below the supply contracts. Specifically the current price target is one 2019 US dollar and price threshold is at 5%. The rebalance occurs gradually over 10 days to smooth out the supply changes, but the key here is that this process is memoryless. The previous days supply change does not matter when calculating the current days supply change. Whatever the supply should be is just adjusted by \\(\\frac{1}{10}\\) everyday. More details can be found in this article written by the founder. Trading Implications This rebalance feature has massive implications for the trading world. It can be very harmful to leave stale bids as they can easily get swept from traders gaming the rebase period which occurs daily at 02:00 UTC. Also it should be possible to predict the rebase direction and benefit from front-running this equilibrium point. Investing Implications Unlike most crypto coins, the price of \\$AMPL is really secondary to the market cap due to the rebase protocol. Below is a market cap and price over the past month from coinmarketcap . We can see that while the price increased from $1 to around $2.3, around a 1.3x increase. However the market cap during this same period increased from 6 million to 120 million, a 20x increase and the true value of the investment during this period. Additional Thoughts Due to the rebase mechanism, \\$AMPL should have very unique returns characteristics, uncorrelated with other assets both in the traditional space and the crypto space. I believe that it is the ultimate momentum asset because as long as the price stays above $1 the supply will increase. People will see that their net worth has gone up and buy more \\$AMPL. Thoertically people will sell the extra \\$AMPL for ever new issuance, but I do not think that will happen at all. People will chase returns and put even more money into \\$AMPL to recieve a larger split. In a market prone to rapid bull runs, it is against human nature to sell. They will infact buy more \\$AMPL, leading to an increase in \\$AMPL supply and potentially causing a large feedback loop. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Crypto","url":"/ampl-intro.html","loc":"/ampl-intro.html"},{"title":"Crypto Intoduction","text":"I have been around crypto since the days of SilkRoad and Mt. Gox. I still remember my college friend buying Bitcoin for under $20 and then ordering edibles off of SilkRoad. Living in a dorm there was free electricity so my suite-mate started to mine Litecoin. That was the early adaptor phase, but after the crash of Mt. Gox cryptocurrencies faded from the mainstream media until the ICO bubbles in 2017. I was working at a trading firm at the time, and the senior traders likened it to the 2000's tech bubble. Anything that you would buy would go up. It was also around this time that the crypto exchanges were very inefficient and prices were not in line at all and I would buy and sell on various exchanges to try to arbitrage the prices, mainly taking on exchange risk. And some of these exchanges were definitely scams like Bitgrail and Cryptopia. By mid 2018, the bubble had burst and crypto was once again headed to the 'dark ages'. A good way to look at the state of the crypto market is a graph of bitcoin dominance, which is the total market cap of bitcoin over the total market cap of crypto assets. Typically during bitcoin bear markets, bitcoin dominance will be higher, as the altcoins have a much larger beta than bitcoin does. We are currently in the middle of a bitcoin winter, with bitcoin stuck between 9-10k, but I am very bullish on the long term possibilities. Take that with a grain of salt as I have been bullish ever since I heard about Bitcoin. Honestly speaking, there are a lot of interesting projects being built right now especially in the DeFi space. I will shed light on interesting projects and look for potential investments in my upcoming blog posts.","tags":"Crypto","url":"/crypto-intro.html","loc":"/crypto-intro.html"},{"title":"UFC Stats","text":"Background Ultimate Fighting Championship or UFC is based off a 10-9 scoring system similar to boxing and the fight goes to the judges unless there is a KO,TKO, DQ, or submission. Typically fights last 3 rounds of 5 minutes, with championship belts and top contender fights lasting 5 rounds of 5 minutes. For those interested, here is the current official MMA unified rules. Note that these have been changed and amended a few times since they were created in the early 2000s. How to Win As mentioned above there are a few different ways to end a UFC fight. Here we see both a pie chart and a time series graph of how fights end. I have removed all DQ and overturned results from the dataset and I have combined the split decision and majority decisions. I aggregate the data into a pie graph and we can see that about a third of the fights are unanimous decisions and another third are KO/TKOs. The other third are a combination of submissions and split or majority decisions. There are rarely any draws. However these percentages are not consistent. Like I mentioned previously there have been many iterations of rules and technique adaptations. The UFC looks vastly different now than in years past. To examine the changes, I create a rolling 5 year time series graph which again contains the various methods of how a fight can finish. The five year rolling method smooths the data and we see that the proportion of submissions and KO/TKOs have both decreased over the years with an increase in both the decision methods. This could hint that fighters are getting better at defense, leading to longer fighting times ultimately more decisions. Data I have built a database of UFC stats (starting from UFC 1 in November 12, 1993 and also incorporating promotions UFC bought like StrikeForce and WEC); in total, this results to a database of over 5000 fights and 3000 fighters. I am able to get fighters physical attributes like age and reach as well as certain fighting stats like knock down percentage. In addition, I have scraped a few betting websites for their lines, but unfortunately some of the earlier fights did not have betting lines. The betting data starts in 2012. I am working to release the scraping code and data on my github so others can create reproducible results and test new theories. When looking through this data it is important to note that the UFC is rapidly evolving and conclusions found today may not hold in a few years. There has been a lot of evolution within the UFC as in the beginning there was no time limits and only finishes, thus draws did not exist. Below is an example of a the fighting stats that I shall collect. Here we see a recent fight between Molly McCann and Taila Santos. Similar to home and away in other sports, there is a red corner (Molly) and a blue corner (Talia) in which the red corner is usually the higher ranked fighter. Taking a glance at the specific statistics, we can see that the UFC records a multitude of stats that are broken down into two categories: striking stats and totals. The totals focus on miscellaneous stats like KD (knock downs), TD (take downs) as well as total strikes. The striking stats are broken down into the location on the body as well as the distance the strike was landed. In addition, strikes are classified as significant or not significant. We see Taila out-striking Molly and outclassing her in the take-down category, so it understanding that Taila won a unanimous decision, Features Going one step further when analyzing these stats, we can use feature engineering to break down these stats on a ratio basis think per minute stats. When Jorge Masvidal knocked out Ben Askren in 5 seconds with a flying knee, he only had 3 significant strikes, the first flying knee and two followup punches. Here is a clip of the fight. Looking only at the total number of significant strikes would penalize this career altering performance. This is an extreme example, but serves as an important reminder as to why changing stats to a common timeframe can be important.","tags":"UFC","url":"/ufc-stats.html","loc":"/ufc-stats.html"},{"title":"Counting Stats","text":"Counting stats, otherwise known as basic stats, have long dominated basketball discussion, because they are easy to understand and the data is easy to collect. Anyone can look at a boxscore and analyze which players had good games or bad games; the issue is that these counting stats mask the process of the ball. They only show the end result and not the process (Embiid shout-out) of how the result was created. Take an example from finance -- below are two theoretical equity returns of two assets which show the percent return from time t=0 to time t=100. Both of these assets return 100% in 100 periods, but they have very different paths to the 100% return. Path 1 is very straight and linear, returning 1% every day for 100 days, while path 2 has a lot more volatility and drawdowns. To an investor who does not care about the paths that the assets take and only the end result/return, the assets serve the same purpose: they return the same net amount. This type of investor is only interested in the asset price at t=0 when he buys it and the asset price at t=100, when he sells it and cashes out. All the information this investor needs can be captured with the two data points t=0 and t=100. This type of investor is similar to a person analyzing the box score, because the boxscore only contains the end result, not what happened in between to achieve that result. However, if a person was to go more granular into the asset returns and look at the day by day paths he would see that the two assets are not the same. I bring this finance example into discussion, because not all counting stats are the same and equal, but they are often discussed as such. By looking at the boxscore, one is able to see certain offensive categories like points, threes, rebounds, assists and defensive categories like blocks and steals. Below is the Warriors boxscore from the Raptors-Warriors game on 2016-11-16. In this boxscore, we can see that Klay Thompson took 11 threes and made 3 while his backcourt mate Steph Curry took 9 and made 3. Suppose that Klay Thompson also took 9 threes and still made 3 of them for the same shooting percentage as Steph Curry. Then it would take a small jump of logic to say that Klay and Steph both shot 33% and had similar shooting nights. This small jump of logic is similar to the fallacy in the two financial assets, because Steph Curry and Klay Thompson create their threes in very different ways: Klay Thompson is a lot more screen dependent then Steph Curry is. When Klay Thompson gets open and shoots a three, often times there are many screeners setting multiple screens to free him up. If Zaza Pachulia sets Klay Thompson a perfect screen that frees him up for a wide open shot and Klay Thompson hits the shot, Thompson gets all the credit in the boxscore. The boxscore will increase Thomspon's FGA by 1 and his FGM by 1. Zaza Pachulia gets no credit. But when Zaza Pachulia gets called for an illegal screen by attempting to free Klay Thompson up, Pachulia gets a TO credited to his box score, while Thompson gets nothing. So Thompson is getting all of the benefits of a Pachulia screen with none of the disadvantages in the boxscore, because the boxscore only focuses on the final result of a make or a miss and not the process. An accurate statistic should incorporate the fact that Pachulia also helped in creating the shot for Klay Thompson and award him appropriately when Thompson hits a shot, while punishing Thompson when Pachulia sets an illegal screen. I bring up Thompsons's backcourt mate Steph Curry, because he shoots many of his three pointers off the bounce which do not require a screener. And because he creates more of his shots from a 1-on-1 perspective, his style is very different than Klay's, like how the two assets have different paths but all lead to the same net outcome. Curry's 1-on-1 playing style (although he does have off ball screens and pick and rolls) creates a more accurate representation of his 3-point shooting percentage when looking at the counting stats of 3PA and 3PM, because the screener is not being accounted for. Klay Thompson shooting around 42% (a very elite number) from three during his entire career, however the counting stats have inflated his number. Suppose that Zaza Pachulia (and previously Andrew Bogut) sets an illegal screen that gets called 10% of the time resulting in a turnover. Instead of hitting 42 threes in 100 posessions, Thompson would instead hit around 38 threes in 95 attempts (but 100 possessions) giving him a real shooting percentage of 38%. This are all made up numbers and a moving screen should not be 100% the shooters or the screeners fault, but this example is just an illustration of how counting stats, while they give a good summary, are not entirely accurate in the picture they paint. Summary: Counting stats paint a slightly different picture than what goes on in a game. Only looking at counting stats neglects the process of the game, just like in finance how only looking at the end result removes intermediary fluctuations. Counting stats create a snapshot in time and summarize an entire possession of time in that snapshot. If Klay Thompson spends 20 seconds running around 2 picks and an elevator screen to get opened, there are multiple people contributing to the final result, however all that gets recorded is what Klay Thompson does. Similarly, a rebound records the player coming down with the ball, but what about his teammates boxing out the opposing players? Shouldn't they get a little credit too?","tags":"NBA","url":"/counting-stats.html","loc":"/counting-stats.html"},{"title":"About Me","text":"I am writing under this pseudonym in honor of my favorite athlete, someone who has inspired and motivated me throughout my life. This blog is a place for me to reason out some of my thoughts in a more coherent manner and to create timestamps for these ideas. The main two topics will be finance and sports. On the finance side, I will write about interesting investments in the traditional markets and in the crypto space. Regarding sports, most of the posts will focus around sports analytics and sports betting. I have been investing/trading for over a decade. In this blog, I want to focus on popular tailwinds and how they may benefit certain industries and players in the upcoming decade. I have a particular interest in tech (more specifically SaaS) sector and will be looking at these companies closely. In addition, I have been around crypto since my college suite-mate introduced me to Litecoin mining. I will be posting my thoughts about the future of crypto, especially as it relates to the exchange and DeFi space. Sports analytics and sports betting are still in the nascent stages. The legal sports betting handle is a magnitude lower than the illegal sports betting handle, which leads to lower consolidated volume and higher vigs. Good clean data to do analytics is even harder to come by. Unlike in finance, where many fin-tech companies entire business model revolve around cleaning and selling data, when dealing with sports the data is hard to come by. For instance, the NBA actively changes their api to prevent scraping. The pipeline requires the ability to web-scrape, store, and clean the data before any analysis can be done. Nonetheless, I have built robust pipelines for the NBA, NFL, and UFC and I will share interesting insights and tidbits on each sport. I will try to open source as much code as possible so others can play around with data and code. I have a statistics background and my day job is a quantitative researcher so my posts will probably lean on the quant side. I can be reached at bobekryant33824@gmail.com.","tags":"About Me","url":"/about-me.html","loc":"/about-me.html"}]};