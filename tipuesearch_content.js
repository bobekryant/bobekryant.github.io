var tipuesearch = {"pages":[{"title":"FTT Mondays","text":"Background After reading the previous article, the follow up question would be when to buy. Luckily, we can use the buyback mechanism as a good entry point barometer and also a stand alone trading strategy. We know there is a buyback that starts after 10pm HKT each Monday and ends beefore 11:59pm HKT Tuesday. In these 26 hours, FTX buys back \\$FTT on the open market giving it synthetic buy pressure, so it is ideal to buy right before this period. Basic Trading Strategy The first buyback was on 08/01/2019 and there have been about 90 more since then 1 . By creating a simple strategy that buys \\$FTT at the initial buyback period and sells as the period ends, we can see how much edge there is when executing before this period. Below is the cumulative return of this period with an average of over 2% per day and a total of over 4x in around 85 days. Trading Strategy Tweaks Timing Improvement There are ways to just make money trading this strategy, but adding a few more bells and whistles to make this strategy more lucrative. One of these possible strategy enhancements is to stalk the network to see when the burn occurs and end the trade at that moment, reducing uncessary delta exposure to the market 2 .. We are able to track the buybacks by looking at large burn transactions . So instead of closing the trade at 11:59pm HKT, we close whenever the burn concludes. Buy Back Size Buybacks can be tracked by the either on etherscan or also on the burn page. After normalizing (in USD) for the size of the burn, we can run correlations 3 to see that there is a strong positive relationship (.36) between the size of the burn and the resulting performance. A trading strategy would be to size up when buy is large and size down when burn is small. Breaking down the USD burn size into quantiles, the following strategy is run: if the burn size is in the upper 75% we 2x the size, bottom 25% half size, and we keep same size for middle two quantiles. Below are risk adjusted stats of the baseline and the various strategy tweaks. We see that the baseline is very strong with an average of 2.3% gain and high sharpe/sortino ratios. The pnl is skewed right as the average winner is higher than the average loser. All in all a very powerful trade. Looking at our adjustments, we can see that conditioning trading size on burn size is positive expected value. We get an increase to 3.2% as well as boosts to sharpe and average winner goes up while average loser goes down. However, the timing adjustment does not seem to help the strategy at all and decreases performance in both outright pnl and risk adjusted metrics. Perhaps there is a momentum effect after this day and cutting off at the burn time does not properly capture the residual momentum. Baseline Sizing Timing Mean 0.023 0.032 0.019 Sharpe 3.990 4.830 3.630 Sortino 15.330 12.920 11.660 Win% 0.610 0.610 0.610 u_win 0.050 0.070 0.050 u_lose -0.030 -0.020 -0.030 best 0.730 0.730 0.630 worst -0.100 -0.190 -0.090 Conclusion A great time to buy \\$FTT is right before the buyback period (10pm HKT on Mondays). The rest of the article examines a basic strategy based on market microstructure and shows the impact of simple tweaks on strategy performance. There are outsized returns during this specific time period because of the mechanics of the $\\FTT coin. Knowing specific details of how other coins behave can help uncover similar trades to this one. This article uses data until mid March 2021. ↩ One week in August 2020, burn occured on 08/14/2020 instead of usual 08/11/2020. ↩ Spearman is used which better accounts for nonlinearity. I believe this would be more accurate here as there are often outsized momentum moves. ↩","tags":"Crypto","url":"ftt-2.html","loc":"ftt-2.html"},{"title":"Voting Pumps","text":"Background DeFi protocol fair value pricing is a mix of modeling technology on its standalone future cash flows, relation value in relation to other DeFi projects, and then game theory to see how incentives (tokenomics) align with the price. This article will focus on the latter game theory aspect: in a particular case of the farming pump. DeFi protocols often use farming (liquidity mining) to bootstrap interest in their project, give the community some tokens, and create liquidity for said token. These can range from safe stable coin farms, single asset farms, to riskiest pool 2 farms. For those who are unfamiliar with the farming craze, a stable coin farm is when \\$USDC, \\$USDT, \\$DAI and other stable coins are deposited in return for a protocol's token. This is pretty risk free as the value of the stable coin wont deviate and then people are just getting the token for free 1 2 3 . A single asset farm is when other assets are deposited in return for a protocol's token. One of the first single asset farms was \\$YAM, when it allowed other DeFi tokens like \\$SNX and \\$YFI to be deposited to recieve \\$YAM. This is inherentely riskier as the tokens from other protocols may increase or decrease in price, removing the risk free value catpure from stable coins 4 5 . Continuing on the risk specturm there are the \\$SUSHI ETH LP token farms and finally the riskiest \\$1inch pool 2 (deathpool) farms 6 . The latter two use liquidity provider tokens from AMMs to create more liquid markets. The Sushi ETH pairing farms are not that much risker from a pricing standpoint as \\$ETH is relatively liquid and typically has lower volatility than the single asset funds, however in deathpool farms one can lose both deathpool tokens. Most of the time, the non-stablecoin farms will have some combination of the popular large coins like \\$ETH and \\$WBTC, and then maybe some DeFi \"Blue Chips\" (\\$YFI, \\$AAVE, \\$SNX). Often times a picture is worth a thousand words, so here is the front end of the sushiswap farm from late august. The APYs vary substaintially from LEND-ETH around 1.24k% to UMA-ETH at 4k% to SUSHI-ETH at 8.7k%. Besdies farming in the SUSHI-ETH pool which has its inflation issues, the next best trade is to farm the UMA-ETH pool because it has the highest APY. It has the higest APY, because all the pools give out the same about of \\$SUSHI per block , but \\$UMA has one of the smallest mcaps so natrually its APY is higher. This caused people to bid up \\$UMA in order to chase the apy and \\$UMA went from $6 to $24. The exact trickle down effect goes something like this: All the pools (other than SUSHI-ETH) distribute the same amount of \\$SUSHI daily. The pools tied to the smallest market cap tokens to have the highest APY. People purchase the coin and try to balance out the APYs between various pools. Coin price rises as the coin is not very liquid in the first place. After that long winded intro on farming, we now come to the interesting portion of various trades one can do during farming. There are certain cases when the members of the protocol get to vote which coins are eligible for liquidity mining and it is in these unique cases that there are often outsized risk/reward trades and the primary focus of this article. Fundamentally speaking these coins that are used for liquidity mining gain a lucrative short term usecase and thus their underlying value appreciates during this time period. I will highlight two different cases below with Harvest Finance and Float Protocol. Harvest Harvest Finance is one of the first Yearn clones and in late August 2020 and they used discord to vote for coins to farm their new token \\$FARM. Each member of the discord is allocated one vote and the results are below. We can see that \\$SUSHI, \\$YFII, \\$YFV, \\$OGN, \\$BASED, and \\$PASTA, ended up winning . For those of you that are unfamiliar with these coins, they are all shitcoins, but some are more trash than others due to their low liquidity. Enter \\$PASTA, a meme food coin with no use case created just over two weeks ago to farm and dump. At the time of the vote, this coin was trading around 1 cent and 100k market cap, but by the time the vote had concluded, it touched 50 cents with a 5 million market cap and 5 million daily volume. The sole purpose of \\$PASTA was as a liquidity mining token for \\$FARM, the Harvest Finance token. Below is the astonshing 50x from 8/31 to 9/2. A closer look at the final tallies reveals that \\$PASTA ended up in 4th place with 109 votes and 6th place went to \\$AMPL with 73 votes. Only 109 discord votes were cast to create this ridiculous pump. Discord accounts require a unique email and emails can be created for free so all this trade required was a few hours of fake emails. Float Protocol Voting has evolved since the early days of DeFi summer and now most projects use snapshot/scattershot which requires the underlying token. A snapshot of all the addresses holding the underlying token is taken at a specific block time and then each token is given 1 vote. It is still is still possible to create large movements in illiquid tokens through the farming pump, but now it will require some risk to capture votes instead of free discord accounts. Float protocol is a unstable coin loosely pegged to the dollar. There was a phase 1 whitelist, in which certain wallets where able to use stablecoins to farm their native token \\$BANK. Then phase 2 allowed the top 5 coins from a vote to also farm. This vote lasted 4 days (from 3/14/2021 to 3/18/2021) and the top 5 vote getters where \\$YFI, \\$YAM, \\$SUSHI, \\$ETH, and \\$wBTC. Out of these 5, only \\$YAM has a market cap under 100 mil and pumped due to its addition as a eligible farming token. The a similar low cap voting pump played out here. On 3/10/2021, a team member from Float created a phase 2 proposal to see which of the new coins would be farming Float's underlying token \\$BANK. After a few days of discussion, the snapshot was taken on 3/12/2021, when \\$Bank was roughly \\$700. There were 3.49k \\$Bank votes (1 \\$Bank equals 1 vote) for \\$Yam and the 5th place token had around 3.26k votes. It would cost rouhgly 2.4 mil for all of the \\$YAM votes 7 . Due to this vote, \\$Yam price went from around 3.5 to a high over 6 and the market cap went from around 40 mil to 70 mil. Not as lucrative in percentage terms as Harvest, but this trade could take alot more size and still was very attractive from a risk rewards standpoint 8 . Below is the price action for \\$YAM. We can see a increase near the close of voting as we see \\$YAM was voted in and then one final spike at the beginning of farming right when farming is about to start on 3/21/2021. Unfortunately the price slowly fell off a cliff as voters decided to shorted phase 2 to 1 week through another proposal, but the initial trade was still very promising. Conclusion There are many examples other examples farming pumps like Harvest and Float where community members can vote in illiquid altcoins for farming collateral. These types of trades should continue to be high expected value trades in the current bull market. It is often during these times that stable coin borrowing rates on AAVE get expensive as people try to borrow stablecoins to farm free tokens at a higher borrowing rate. ↩ Many of the first DeFi farms were stable coin farms like Yearn. ↩ There is always smart contract risk. DYOR before interacting with these farms and be sure to revoke permissions afterwards ↩ For this extra risk, the yield on these single asset farms are often times multiples higher than stable coin farms. ↩ Often times yield farmers will use futures on centralized exchanges to hedge out the delta risk of the various single asset tokens, thus creating a synthetic dollar and getting the yield \"risk free\" ↩ If there is interest I can write another article on the intracacies of the various farms as there is also a decent amount of game theory at play there ↩ It would cost around 2.3 mil for 5th place. ↩ There are also no futures of \\$YAM so is hard to get a short and farm risk free. Thus the \\$YAM pool would have alot less TVL and thus higher APY. ↩","tags":"Crypto","url":"voting-pump.html","loc":"voting-pump.html"},{"title":"Bullish Hack","text":"Background DeFi protocols get hacked all the time: Yearn, Pickle, Alpha, are among some of the more famous victims and this list goes on and on. Recently, DODO exchange, another AMM, also got hacked . Lucky for DODO, this was a white hat hacker who returned most of the funds. One of the main projects involved in this hack is CrescoFin and their token $WCRES. The token dropped over 50% before gradually recovering to down around 20% of its prehack levels. AMA In response to the hack, the CrescoFin team released a AMA where they addressed the hack and future plans for the protocol. Derek, one of the founders, claims \"We will deploy an initial tranche of 200 ETH over the next 7-10 days and then evaluate the results.\" This is less than half of the total ETH received , and it seems they will market buy $WCRES in a TWAP manner. Trade I view this as a clean event trade with the fact that the price is around 20% off of the prehack price and the forced buy back. In that same message Derek states \"We also believe that the organic buying from existing and new tokenholders will help fill some of the gap, or just shoot past the pre-hack level of $9/0.005ETH.\" He clearly believes that the token holders were wronged and seems like the team might use the additional 211 ETH to push the price to his targets if they are not reached with the first batch of buybacks. Doing some quick back of the envelope calculations, on the WCRES/ETH pool, we see that there is only around 3.5 million in liquidity and close to 200k volume daily 1 . Market buying 200 ETH would create about 10% in price enhancement. Judging by the message above and being conservative, the team plans to market buy 20 ETH each day for 10 days. With ETH around 2k, this will create around 40k daily additional buy pressure; a huge portion of the total 200k traded volume. I expect \\$WCRES to increase during this period and am putting on this trade at around $7 and .004 WCRES/ETH. A decent amount of this volume is probably arbitrage bots going from the DODO WCRES/USDT pool to the Uniswap WCRES/ETH pool. ↩","tags":"Crypto","url":"wcres-hack.html","loc":"wcres-hack.html"},{"title":"FTX Exchange","text":"Background This is the first of many posts where I lay out my fundamental thesis for altcoins. Investing in these altcoins coins is risker than \\$BTC and \\$ETH as they have higher betas, but the upside is enormous. The return profile is similar to VC who investing in tech startups: most of the projects go bust, but a few of the winners can pay for all the losers and more. Unlike in traditional startups who adjusts valuations between rounds, crypto altcoins have constant price feedback, creating a lot of volatility especially at the lower market caps. Altcoins The altcoins can be split into various sections similar to how MSCI splits up the S&P 500 into 11 different subsectors like financials, energy, and utilities. Looking through the top projects, there are a few groupings that jump out: Layer 1's (\\$ETH, \\$DOT, \\$ADA, \\$SOL) Stable Coins (\\$USDT, \\$USDC, \\$DAI, \\$SUSD) Exchange Coins (\\$BNB, \\$FTT, \\$KCS) Privacy Coins (\\$XMR, \\$ZEC, $\\SCRT, \\$PRIV) DeFi (\\$LINK, \\$UNI, \\$AAVE, \\$SNX) This is not an inclusive list and certain projects can be in different groupings like \\$UNI is both DeFi and an exchange coin. We group the tokens in subsectors to make it easier for relative value comparisons of price, cash flow, and growth. FTX In this article, I focus on \\$FTT, the native token for the cryptocurrency exchange FTX . FTX is one of the top cryptocurrency exchanges by trading volume with lots of innovative features that make it stand out: They were the first exchange to create 3x and -3x levered tokens 1 2 . Tokenized stock trading including pre-ipo stocks which trade 24/7. FTX participates in the IPO process of certain companies and receive shares from underwriters. They then use those shares to create markets before official IPO date. 3 They are often the first exchange to list new tokens. Examples include \\$USDT and \\$SUSHI futures. Consistently have quality IEOs (initial exchange listings). Like IPOs in the traditional market or ICOs from 2017, nowdays many protocols directly list on exchanges. FTX vets these protocols and allows exposure from day one. There is a world class risk management system with minimal clawbacks, unlike some of their competitors. This allows them to capture more volume in times of high volatility. There are a multitude of other reasons to be bullish on the FTX exchange capturing large amounts of the volume like single currency collateral and a responsive social media team. In terms of size, FTX only started in mid 2019 and is now around 5th in terms of derivative volumes. Specifically in the spot market, they went from around 10 million to 300 million daily volume and the derivative market grew from 300 million to over 6 billion daily volume in 2020. \\$FTT In summer 2019, FTX launched their exchange token \\$FTT which provides trading discounts and other airdrops to holders of the token. Staking \\$FTT tokens can give maker rebates up to .3 bps and FTX is one of the few large exchanges with maker rebates often leading to deeper liquidity on the exchange. In addition there are weekly buyback and burns of the tokens until half of the supply is destroyed. These burns equate to 33% of all fees generated from the exchange 10% of net additions to the insurance fund 5% of fees from other uses of the platform Tokens are bought starting around 10pm HKT each Monday, with purchased tokens burned by 11:59pm HKT Tuesday. ICO There were 3 ICO rounds which sold a total of 59.3 million \\$FTT tokens sold. The breakdowns are as follows: 50 million tokens between \\$0.1 and \\$0.2. These linearly vest over first 3 months. 6.5 million tokens between \\$0.2 and \\$0.6. These linearly vest over first 1.5 months. 2.8 million tokens between \\$0.6 and \\$0.8. These linearly vest over first month. Token Emission and Vesting Schedule There is a 350 million supply cap of \\$FTT according the the whitepaper 4 . Currently there are around 100 million \\$FTT circulating, so it is important to learn the emission schedule as a lot of these tokens could be coming on the market creating dilution and sell pressure. Apart from the ICO tokens described above, there are around 115 million tokens to be sold for future rounds and 175 million tokens for other tasks like insurance fund insurance, safety fund, liquidity fund, and team tokens. These are all locked up over 3 years with a linear vesting schedule. Circulating Supply Here is the genesis wallet which held 350 million \\$FTT. Since then, 273 million has exited leaving the wallet with 77 million \\$FTT. Those 77 million tokens are presumably part of the 115 million tokens to be sold for future rounds. On Crunchbase , we can see that FTX raised two additional rounds, in December 2019 and March 2020. I believe that 38 million tokens were sold already with a three year daily unlock. Another large wallet has over 170 million \\$FTT and it is the companies wallet. This leaves a total of around 100 million circulating tokens. Inflation and Deflation Modeling Weekly Unlocks On the transparency page , there is a link to the breakdown of team tokens. The team has around 31.25 million tokens locked over 3 years translating to about 200k tokens unlocked a week. The other two rounds of investors who sold on Crunchbase have around 38 million tokens locked over 3 years translating to about 240k tokens unlocked per week. Summing those two together, there are around 440k tokens unlocked each week. Token Burns The opposite of the inflation schedule is the weekly token burn. At the time of this article, there have been almost 10 million tokens burned which equates to around 100 million dollars. The best proxy for the tokens burned is trading volume and we can create a model predicting the amount of tokens burned to the volume traded. Around 100k-200k tokens were burned per week in 2020, so this would lead to anywhere from 200k-300k new tokens introduced per week or around 10 million to 15 million per year akin to 10% to 15% of the circulating supply. Pricing One of the more fundamental ways to see if the price of a specific cryptocurrency will go up is to look at the supply increase and see if the underlying fundamentals will increase more or less than the specific supply. We see that there will be 10-15% of token dilution so we should see if the fundamental metrics will increase more of less than this number as this would keep some of the fundamental ratios in line. For example, if we were valuing the company like a traditional stock on P/E ratio in order for the price to go up over the 10-15% dilution, the earnings must also go up 10-15%. This leads us to the next section which we can project the fundamentals of FTX which is again (inherentely) how much volume will trade on FTX. Crypto Volume At the heart of the future valuation is the projected future volume. Cryptocurrencies are a growing industry and volume have slowly increased throughout the years. The exchange pie is getting bigger and FTX is being a larger player relative to other exchanges. Below we a picture of aggregated futures crypto traded volume from 2020. Specifically the blue line is the volume traded 5 , which went from around \\$20 billion a day to the current \\$160 billion a day an 8x increase in a year. In addition, the red line is the plot of futures volume 6 traded on FTX. This started at 2\\% and has since gone up to close to 4\\% of total derivatives volume 7 . I expect both the volume of crypto currencies traded and the percent traded on FTX to continue to increase in the foreseeable future. So this past year FTX almost doubled its volume in relation to its peers and crypto itself did an 8x in volume. If 2021 is even remotely close to 2020, \\$FTT is extremely undervalued. Conclusion FTX has consistenly made strides to capture the most crypto volume. In August 2020, FTX bought Blockfolio , one of the leading portfolio tracking apps with over 6 million users at the time of purchase. Then just yesterday, on the heels of Robinhood and other US brokerages limiting stock trading in select names, Blockfolio adds zero fee stock and crypto trading . With innovative features along with a history of growth, I believe that FTX will continue on its path to becoming the largest exchanges in the world. I own \\$FTT both as a beta play on crypto exchange volume increasing as well as an alpha play on FTX continuing to eat market share from other participants. The pie is growing and the share of the pie is growing. This is similar to the levered ETFs in traditional finance. ↩ These have the advantage of having fixed borrow compared to futures and there are no liquidations on the levered tokens. ↩ Airbnb was the first pre-IPO market and Coinbase is the second. These provide a unique way to get exposure to an unlisted but popular stock. ↩ Etherscan says there is 340 million but I will lean on the whitepaper number. ↩ Rolling 30 days ↩ Also rolling 30 days ↩ The reason why there is the spike in February and March is because that time was very high volatility and FTX was one of the exchanges that was still functioning properly with the least amount of clawbacks. ↩","tags":"Crypto","url":"ftt-1.html","loc":"ftt-1.html"},{"title":"Bitcoin for the Portfolio","text":"Background I am a Bitcoin permabull, so my views on Bitcoin are definately biased. There are numerous fundamental reasons why I believe it is poised to revolutionalize the financial industry as we know it, but I will save those arguments for a future post. This article will use statistics to show why allocating bitcoin to your portfolio has historically been beneficial on both an absolute and risk adjusted basis. The traditional retirement portfolio is the 60/40 portfolio, where 60% of the portfolio is in various equities and 40% is in some form of fixed income. Typically portfolio managers rebalance this 60/40 portfolio quarterly to keep the same exposure to both asset classes. Using \\$SPY and \\$TLT as proxies for euities and fixed income, we can simulate the performance of a 60/40 portfolio during this decade long bull market. The following plot shows a 60/40 portfolio with \\$1000 dollars starting in 2014 till present day. This portfolio had pretty consistent performance until the Covid crash in 2020 and there is strong risk adjusted performance. In addition, I show what would have happened if instead this portfolio had 2%, 5%, and 10% exposure to \\$BTC in addition to the equity and fixed income exposure. I remove equal exposure from the two assets so at 2% \\$BTC exposure, we also have 59% equity and 39% fixed income exposure. Total Return CAGR Max_dd Sharpe Sortino 0 1.17 0.10 -0.18 1.04 1.06 2% 1.43 0.12 -0.18 1.16 1.22 5% 1.86 0.14 -0.20 1.23 1.30 10% 2.67 0.18 -0.22 1.19 1.28 The more \\$BTC we add to a portfolio, the better it does from an absolute return perspective. For the risk adjusted performance, it seems that 5% BTC with 37.5% fixed income and 57.5% equity exposure was the best. The above statement probably seems obvious in hindsight as \\$BTC went from less than \\$1,000 to over \\$16,000 during this time period. What I want to stress is that including \\$BTC in the portfolio increases the risk adjusted returns of the portfolio by over 20%. Crypto Bear Markets Bitcoin has numerous periods that it has significant drawdowns as it is a very volatile asset. However due to a phenomena known as Shannon's Demon or volatility harvesting, a portfolio including Bitcoin can still outperform one without Bitcoin during these drawdown periods. This can be easily visualized if we start the portfolio with 1000 dollars at all time high of \\$BTC of 19,000 on December 18th, 2017. On this date, \\$SPY is around 250 and \\$TLT is around 120. Fast forward to today and \\$BTC is around 16,300, \\$SPY is at 360 and \\$TLT is hovering around 160. Absolute return wise, both \\$SPY and \\$TLT outperformed \\$BTC. However, when we add \\$BTC to the rebalanced portfolio, we see that the portoflio with the best relative and absolute performance is actually the one with the most \\$BTC exposure at 10%. Total Return CAGR Max_dd Sharpe Sortino 0 0.46 0.19 -0.18 1.39 1.37 2% 0.52 0.21 -0.18 1.51 1.54 5% 0.59 0.24 -0.20 1.61 1.64 10% 0.72 0.28 -0.22 1.65 1.66 Shannon's Demon \\$BTC works well in the above portfolio, because it is an uncorrelated asset. Although it produces a negative return on its own, there are large diversification benefits to adding it to a equity-fixed income portfolio. The power of diversification and rebalancing is shown clearly in the following made up example: Suppose there is an asset that either doubles or halves at every timestep. In addition, it starts and ends at the same price. In the example below, the price of this asset starts and stops at 100, but by rebalancing, the price of the portfolio ends at over 600, more than 6x the original starting point. Obviously this is an extreme example, but it properly illustrates the power of reblancing under a mean reverting asset. We can use monte carlo simulations to look at more realistic scenarios of possible performance from reblanacing. Rebalancing is not a magical panacea and there are places where it underperforms not rebalancing. I generate 1000 synthetic streams from a random gaussian process with 250 datapoints, setting the mean to zero and an annualized 20% standard deviation. Each day I do a 50-50 rebalance with cash and then compare the performance against a non-rebalacning portfolio at the end of the time period. As we can see in the below graph, rebalancing a portfolio gives a payoff similar to selling options on the underlying. When the underlying does not move that much, rebalancing is plus ev, but if there are extreme moves then it is negative expected value. We see that when the asset moves less than 20% in either direction, rebalancing is better than holding. Out of 1000 iterations over 2/3rds of them did better when rebalancing. Conclusion Rebalancing assets is a powerful way to increase both absolute and relative returns. \\$BTC is the ultimate asset for a traditional portfolio, because it is uncorrelated to traditional assets. Readers should consider adding a small portion of portfolio to Bitcoin and periodically rebalance the new portfolio.","tags":"Investing","url":"btc-portfolio.html","loc":"btc-portfolio.html"},{"title":"Election Odds","text":"Introduction There are numerous prediction markets 1 which are implemented to trade the outcome of events like if Donald Trump will win the 2020 presidential election. These markets are different from traditional financial markets because they usually have a defined end point where the bet settles to either 0 or 1 (similar to sports betting). Due to the nature of the settlement, there is extreme volatility as new information gets processed and odds rapidly shift. Currently, the most popular prediction markets are the political events like the recent 2020 presidential election; in fact this election has generated the most volume of any event in the history of many bookies. Other less popular markets include world events like \"if Kim Jung Un will still be North Korea's leader by the end of this year\" and if \"2020 will be the hottest year on record\". They can range from \"if Kim Kardashian and Kayne West will get a divorce in 2020\" or \"if Airbnb will begin publicly trading in 2020\". Some of the most popular prediction betting sites include 2 are PredictIt or the decentralized crypto alternatives Augur and Polymarket . Oftentimes these sites will have similar markets, so arbitrage is possible between the sites or even within the various markets in the site. Before betting it is important to read the fine print as to how the bet settles as they could slightly differ from site to site. In addition different sites have different betting and payout fees. PredictIt has a 5% withdraw fee and a 10% winnings fee. Augur uses base layer Etherum and will have expensive gas fees along with around 1-2% of winning fees. Polymarket charges around 2% of trades because it uses an AMM (Automated Market Maker) similar to Uniswap and its bonding curve. 3 It is worth noting that PredictIt is capped at \\$850 dollar max risk per bet in order to comply with regulations and users need to KYC. They try to get around this by offering multiple ways to bet the same outcome like \"Will Donald Trump win the 2020 presidential election\" along with \"Will the republican nominee win the 2020 presidential election\". Augur and Polymarket do not suffer from these constraints as they are decentralized, but they have less volume and liquidity than PredictIt. Election Primer It is shocking how much volume and liquidity there is for these some of these events, because they are often mispriced relative to other correlated bets as well as in absolute probability. Below is a screenshot of the Polymarket presidential market and the correlated markets taken November 6th 2020. Note that a yes of $0.12 means the yes outcome has a 12% chance of occurring. Joe Biden, the democratic contender, is on the eve of winning the election and the only ballots left to count are the mail-in ballots and provisional ballots which heavily lean democrat. At this point, most of the battle ground states (Florida, Michigan, Wisconsin) had been called 4 , or about to be called. For all intensive purposes, Joe Biden wins this election if he wins either Georgia, Arizona, or Pennsylvania whereas Trump would need all three states. Election Modelling Relative Probabilities Instantly it is obvious that something is wrong with these odds. If we use relative probabilities implied from the state odds, then we can calculate Biden's chances as follows: \\begin{align} P_{Biden} &= 1-((1-P_{ADem})*(1-P_{GDem})*(1-P_{PDem}))\\\\ P_{Biden} &= 1-(.27*.23*.12)\\\\ P_{Biden} &= 1-(.0075)\\\\ P_{Biden} &= .9925\\\\ \\\\ \\text{where} & \\\\ P_{Biden} &= \\text{Probability that Biden wins the election} \\\\ P_{ADem} &= \\text{Probability that Biden wins Arizona} \\\\ P_{GDem} &= \\text{Probability that Biden wins Georgia} \\\\ P_{PDem} &= \\text{Probability that Biden wins Pennsylvania} \\\\ \\end{align} Assuming the probabilities of winning each state are independent, the probability that Biden wins the election is the reciprocal of the probability that he losses all the states. Nonetheless, some simple hard arbitrage betting strategies can be created like buying Biden winning the presidential election and Trump winning Pennsylvania. In this scenario, if Biden wins Pennsylvania, he automatically wins the presidential election and the market is hedged, we make \\$.12 off Biden's presidential victory and lose \\$.12 off Trump's Pennsylvania victory. If Biden losses Pennsylvania, then we make \\$.88 off Trump's Pennsylvania victory and can potentially still win the presidential election, but even if he loses the loss is still only \\$.12 for a total profit of \\$.74. Absolute Probabilities Pollsters often use advanced models to try and predict the probabilities of the election race. Some of the more popular/famous ones include Nate Silver, who famously predicted all of the races correctly in 2012 and 49/50 correctly in 2008 . How these models build fundamental predictions from the bottom up is more advanced than the scope of this blog, but the gist is that they take state polls and try to adjust for various polling biases like education and race. Here is a video of him calling the Pennsylvania race 1/20,000 chance for Trump making both the Biden Presidency and Biden Pennsylvania lines very mispriced. At this point, Biden was ahead of Trump with only mail-in and provisional ballots left and Nate Silver like most political analysts saw no chance for Biden to lose, but the market had Biden only at 88% to win the election. Conclusion There is a lot of edge in these prediction markets both from an absolute value perspective and from a relative value perspective. These markets have started to generate a lot of interest and I expect volume and liquidity on these platforms to grow in the coming years. I see modelling these markets as no different than sports betting because they both have defined payouts at fixed ending times. Sports betting is a subsection of prediction markets. ↩ Other than the sports betting sites ↩ I will dictate a future series of posts to AMM's as they have very interesting principles. ↩ Biden's margin of victory in Wisconsin was below the automatic threshold so there would be an automatic recount triggered by state law, but in the history of recounts those have not changed the final tally by over one thousand whereas his margin of victory in Wisconsin was over twenty thousand. ↩","tags":"Prediction Markets","url":"prediction-markets.html","loc":"prediction-markets.html"},{"title":"Quantifying the Expected Value of a Second","text":"Introduction Basketball analytics have come a long way in recent years. A lot of work has gone into \"efficient basketball\" (dubbed Morey-ball by some) on how to optimize shot distribution. This has led to teams sacrificing the midrange shots for a layup or a three-pointer. In this post, I wish to do a deep dive into a lesser explored topic: the shot clock and how it can be used as a decision making tool to play optimal basketball. For instance, a semi-contested three with 20 seconds left on the clock is probably not an ideal shot, but what if there is only 5 seconds left on the clock? Is there a way quantify shot quality as it relates to the shot clock? Background Knowledge Shot Clock In the 1954-1955 NBA season, the NBA introduced the 24-second shot clock to speed the game up. The shot clock resets when there is a clear change of possession by either a defensive rebound, a turnover, or a made basket. If there is a kick ball violation, foul, or offensive rebound 1 , the shot clock resets to 14 seconds. Teams have at most 8 seconds to get the ball across the half court line, so there is always at least 16 seconds in the 'half-court' setup. Points Per Possession Every offensive possession has a non-negative expected value, ranging from 0 to 3 2 . A team can miss or turn the ball over in which case they score 0 points this possession, or hit a three-pointer in which case they score 3 points. Teams will have different expected points per possession (PPP), and certain players are more efficient scorers than others, but in large part the PPP of a team ranges from about 1-1.15. Since teams typically have around 100 possessions per game, a .1 difference in PPP is very large and over the course of the game ends up being 10 points, the difference between a lottery team and a championship contender. Thesis Assume the Lakers are expected to score 1.2 points every possession and we denote this expression as $E[PPP|\\text{24 seconds}]=1.2$. It is important to understand that this 1.2 number is with a full 24 second shot clock and should fall as the shot clock declines. Taking the extreme, if there was only 1 second left on the shot clock, the $E[PPP|\\text{1 second}]$ would be strictly less than 1.2 (probably more like .5) 3 . I wish to quantify the difference in $E[PPP]$ as it relates to the shot clock, so every second on the 24 second shot clock will have a different $E[PPP]$. My working hypothesis is that there is some sort of logarithmic distribution from 0-24 seconds. The $E[PPP]$ will be low in the beginning and then gradually rise and top off at 24 seconds. Below, I have created a model of what I believe the trade-off should look like 4 . With 1 second left on the shot clock, anything thrown to the rim is better than an alternative of a turnover so I think $E[PPP|\\text{1 second}]$ is close to the expected value of a contested fade-away. At 24 seconds on the shot clock, we have the full 1.2 number, but the relationship should not be a linear one. NBA teams typically do not need the full 24 seconds to shoot and can probably generate a decent shot with half the time. More time just gives them the optionality to turn down good shots for great shots. I use an inverse exponential model which tails off later in the shot clock. Data I was unable to get recent up to date NBA shot clock data as it seems the NBA back-end does not show shot clock data, but it still has a bunch of other (harder to collect) stats like closest defender distance and location on court. If anyone knows how to collect this data please get in touch, I would love to use up-to-date data 5 . However, I found historical shot-log data 6 which included shot clock for the 2014-15 season. This data includes over 100,000 shots with at least 1000 data points for each second of the shot clock so it is decently robust. Although NBA teams have gotten more efficient over the past few years, I believe the overall relationship between PPP and shot clock should remain stable. After doing some data cleaning like removing end of quarter situations when the shot clock is turned off to avoid rushed heaves, I group the data by second and look at the $E[PPP]$ at each of these seconds to create the plot below. I shall refer to this plot as the PPP trade-off curve. We see a similar shape to my predicted curve, where at 1 second, the PPP is below .6 and at 24 seconds the PPP is around 1.2. An interesting observation is that the PPP peaks at around 22 seconds as opposed to 24 seconds, and I suspect this has to do with offensive rebounds and immediate put backs. The shots with 23 and 24 seconds left on the shot clock are offensive rebounds and thus contested close to the basket shots, whereas the shots with 21 and 22 seconds left on the shot clock could be great transition looks as it takes a few seconds to get down the court 7 . Also, it is interesting that there is a slight increase in efficiency around 12 and 13 seconds left on shot clock as opposed to 14 and 15, which could be due to the fact that shot clocks reset to 14 seconds after fouls. This then enables a team to run a scripted out of bounds play which could lead to a cleaner look and more efficient shot. Game Adjustments I want to clarify that these numbers are in the context of generating good looks. Just because people score around 1.1 PPP when there is 24 seconds left on the shot clock does not mean launching full court shots is a winning basketball strategy. There are a lot of extensions that can be built upon this analysis, but it hints that NBA teams should always be pushing the pace to give them more opportunities to generate a clean look. Often times we see point guards just casually bring the ball up the court, but they should probably be sprinting as every second is valuable. From a more mathematical perspective the PPP trade-off curve can dictate optimal basketball strategy, like when to swap a good shot for a great shot, which play to run, and even which players should be on the court. When to shoot? In an optimal world, teams are able to calculate the expected value of any shot using a few important features such as: Who is shooting the ball? Who is the closest defender? What is the distance of the closest defender? What is the shot location? How much time is left on the shot clock? This information can then be extrapolated and used as a decision making tool. If the player believes that his current shot is highest expected value his team is going to get this possession then he should shoot it; else he should turn the shot down. This is a very hard decision, because players do not know the expected value in the future; at best the player holding the ball can pass to a teammate and calculate his teammate's expected value of a shot using the same criteria above. Besides the one pass ahead calculation (which will have a lot of variance), most of the time a player just would not know the future value of this possession, so it brings a game theory question of what to do with the ball. A player can rely on the PPP trade-off curve to aid in his decision making process. On a broad level, if a shot creates more expected value than the $E[PPP]$ at the given shot clock time, a player should probably shoot the ball. This provides his team with higher than average PPP which by definition is a winning strategy in the long run 8 . Using the same graph as above, I shaded the green area to represent optimal area to shoot and shall refer to this area as the green zone. Most of the time the optimal strategy is to take the first shot that is in the green area, as the expected value of the possession decreases the longer a team holds the ball. I say most, because as mentioned previously there are occasions where the extra pass leads to an increase in $E[PPP]$ and players need to weigh this trade off carefully. Coaches Play Calling Coaches can also follow the trade-off curve when calling plays. The Warriors are actually a prime example of a team successfully implementing the PPP trade-off curve, even though they never formalized this concept. For the majority of the shot clock, the Warriors will run a variety of screens to free up Curry or Thompson to get them a comfortable shot. If the plays are successful in creating a clean look, the shot would instantly be in the green zone (regardless of shot clock) as both are prolific 40%+ shooters from deep. A classic example is this elevator screen where three other Warriors players work in unison to get Curry an open look. Lets take a closer look at the elevator screen referenced above. Notice the shot clock is at 7 seconds during the initial pin-down and then Curry catches it with about 4 seconds left. A wide open Curry three is worth at least 1.2 points, but what if the Jazz switched and blew this play up? Then Klay would be isoed against Gordon Hayward 30 feet from the hoop with 3 seconds left on the shot clock, and I estimate this to probably be around .6 points. Using the PPP trade-off curve as a guideline, one can judge if the elevator screen was a good play call with 7 seconds left on the shot clock. Let us assume that the elevator screen play takes 4 seconds to run. We can then formalize the question: if the expected value of the elevator screen play plus the expected value of the 3 seconds after that are greater than the $E[PPP|\\text{7 seconds}]$, the elevator screen is a good play to call. Let us break the outcome of the elevator screen play down into two states, the state that Curry gets a clean look and the state that Curry does not so Thompson keeps the ball. This would lead us to a formula to calculate $E[\\text{Elevator Screen}|7 seconds]$. \\begin{align} E[\\text{Elevator Screen}|7 seconds] &= P(ES)* E[\\text{Curry 3PA}] + (1-P(ES))* E[\\text{Iso}|\\text{3 Seconds}] \\\\ \\text{where} & \\\\ E[\\text{Elevator Screen}|7 seconds] &= \\text{Expected Value of the Elevator Screen given 7 seconds left} \\\\ P(ES) &= \\text{Probability elevator screen is run successfully} \\\\ E[\\text{Curry 3PA}] &= \\text{Expected value of Curry getting a relatively clean look from three} \\\\ E[\\text{Iso}|\\text{3 Seconds}] &= \\text{Expected value of Klay isoing given 3 seconds left} \\\\ \\end{align} If $E[\\text{Elevator Screen}|7 seconds]=E[PPP|\\text{7 seconds}]$, then the play was an average play, and the delta between these variables will determine how good or bad of a play it is. Maintaining the prior assumptions that a Curry three is worth 1.2 points, a Klay iso (with 3 seconds left) is worth .6 points, and $E[PPP|\\text{7 seconds}]$ is .9 points, I will vary $P(ES)$ values to determine the possible range of $E[\\text{Elevator Screen}|7 seconds]$. Looking at the graph below, I plot two lines $E[\\text{Elevator Screen}|7 seconds]$ and $E[PPP|\\text{7 seconds}]$ to see at which point the trade off occurs. So to answer the original question, if the Warriors can get the $P(ES)$ above .5, then the play is good, as they will be in the green zone. If they cannot then they should run a different play with 7 seconds left. Decision Making Detailed The above example is a scenario in which the primary option was open, but it is not always that simple. Sometimes the defense rotates to recover and it is during these instances the PPP trade-off curve can be used effectively. Here we have a couple of screens that do not lead anywhere until a Thompson Green pick and roll with 10 seconds left in the shot clock. Draymond Green catches the ball wide open with six seconds left on the shot clock and the picture below captures this exact moment. Green ends up turning that shot down to pass to Livingston under the hoop and I argue that this was a suboptimal decision with respect to the PPP trade-off curve. If he shoots the ball there, it roughly translates to 1.16 points given that he was shooting 38.8% from three that year and he is wide open. This 1.1 number is much higher than the typical points expected with 6 seconds left in the shot clock, so he probably should have shot it. The counter argument is if he thinks Livingston under the hoop versus Iman Shumpert is a higher expected value than 1.16 points which would mean Livingston would have to shoot above 58% on his shot, which I do not think is likely. Just pulling up Livingston's shooting splits and his proximity to the hoop which I estimate at around 5 feet 9 , I see that he shot around 45% from that distance, giving an expected value of 0.9 points on that shot. This play by Draymond ends up turning into a positive because Lebron inexplicably doubles, leaving JR Smith to fend off Thompson and Barbosa. JR Smith then compounds Lebron's mistake by leaving Klay Thompson one of the greatest shooters in NBA history open to contest Barbosa, a career average 3-point shooter. Tough Shots Finally, certain players are better at scoring tough shots than others. This skill is not very useful when there is 20 seconds left on the shot clock, but is very useful when there is 5 seconds off the shot clock. Many times plays do not work, counters to plays do not work, and the last resort is to throw it to a player and let him create something out of nothing. Going back to the Warriors, Kevin Durant was notorious for the bailout call, where he would get the ball with less than five seconds left and shoot a contested jump shot efficiently. His ability to hit difficult shots at an elite clip complimented the Curry and Thompson screens and this is why the Warriors had one of the best offensives in the history of the NBA. Final Thoughts Current boxscore basic and advanced stats come up short in evaluating tough shots, because they group the shots together without looking at circumstances such as shot clock. Shooting 45% on tough shots is not efficient when there is plenty on time on the clock, but it is elite if there is only a few seconds left. There should be some metric that quantifies the ability to score while the shot clock is low. If a player holds the ball for a long time or drives to the hoop without creating any shot, he is actually harming the expected points of the possession. However, this player receives no direct negative statistic in the boxscore. Another variation of this is when teams spend a majority of the shot clock to post up a player and the pass never gets thrown, either due to poor positioning by the guy in the post or the person throwing the ball. One of these players should be penalized for the seconds wasted but this does not exist in current stats. The exact counter point is when the defender shuts down a drive. This defender does not receive any accolades for chopping seconds off the shot clock as the play must now reset. Teams can go through a game and rate every possession to see when a shot was taken in the green zone and when a shot was passed up in the green zone. They can then attribute these plays to individual players, to see which players make good decisions and vice versa. Offensive rebounds use to reset to the entire 24 second shot clock, but this was changed for the 2019-20 season. ↩ Technically, there is the ability to get a 4 point play with a three pointer plus a foul or even 5 points with a flagrant foul and a three pointer. ↩ This is inherently due to the quality of the shot when the shot clock winding down being worse. ↩ These values are rounded up, just like on the NBA shot clock so even if there is .1 seconds left on the shot clock it is counted as 1 second left. ↩ An alternative would be to parse the play-by-play data, and code up logic to calculate the shot clock, but I am trying to avoid that path for now. ↩ It is worth noting that the data unfortunately only includes field goals and not free throws or turnovers. ↩ I believe that using the new 2019-2020 season data which resets shot clock to 14 after offensive rebounds will change this shape dramatically with regards to the 23 and 24 seconds shots. ↩ Teams can customize the PPP trade-off curve to add in their own percentages as they may be more or less efficient than league average ↩ The entire paint is 12 feet wide, half of that would be six feet and he looks to be slightly inside the paint ↩","tags":"NBA","url":"shot-clock.html","loc":"shot-clock.html"},{"title":"Nexus Mutual","text":"Introduction Decentralized finance, also known as DeFi, is the hottest subsection of crypto right now and is one of the main driving forces behind this current the bull market. In this space I have two tokens with high conviction, Nexus Mutual, \\$NXM, and Yearn Finance, \\$YFI and I my thesis for Nexus Mutual is below. Here is an outline of the article: The use case for \\$NXM. The tokenomics and pricing model behind \\$NXM. Alternative data and price predictions. Recent protocol changes and why I am so bullish on it. DeFi The main principles of DeFi focus around rebuilding the financial system in a more fair, equitable, and transparent fashion. Visible and auditied immutable smart contracts are deployed so users generate trust the in the system and its underlying code. There are many companies tackling various problems across the entire DeFi stack. Currently, the projects in DeFi with the highest market cap are trading (decentralized exchanges) and yield farming (lending). Smaller projects include prediction markets and payments are also popular. The amount of assets in this space has seen staggering growth in the past few months. A popular metric to measure this growth is TVL or total value locked. This translates to the sum of the value of the assets in the various DeFi platforms. Below is a picture of the TVL for the past year from DeFi Pulse . We can see that the TVL had a steady increase from March to July, but since then the TVL has taken off like a rocket ship, going from 2B in assets to over 6B. 1 2 NXM Use Case: Insurance At the heart of DeFi is a reliance on smart contract protocols. Nexus Mutual provides insurance on the underlying code to protect users from bugs in these contracts. Nexus Mutual is the first and largest insurance provider in the DeFi space. The current smart contracts in Nexus Mutual cover the popular DeFi tokens such as \\$CRV, \\$YFI, \\$SNX, and \\$LEND. Members of the mutual are able to use the \\$NXM token to provide coverage of smart contracts, purchase coverage of smart contracts, and participate in claims assessment and risk assessment. The \\$NXM token can be purchased here 3 on the Nexus Mutual exchange. If DeFi grows, Nexus Mutual should also grow as people want to exposure to this space, but also need to cap downside risk. Below is a scatter plot which shows the empirical relationship of the TVL in DeFi to the Nexus Mutual capital pool (amount of assets in the mutual). The data starts in July 23, 2019 and goes to present day. Although the data is noisy (each data point is a day), there is a strong linear and then exponential trend between the two variables. NXM Pricing Model \\$NXM does not follow normal free market pricing like the other tokens. It has a continuous token model also known as a bonding curve. This means that the price is formulaic and varies based on two main parameters: Funding level of mutual Amount of capital required to cover claims In the short term, the funding level of the mutual matters as this impacts the immediate financial position and encourages new money when funding is low. For the long term, the required capital rises to reflect platform adoption. NXM Pricing Model: Bonding Curve Formula The bonding curve formula is a little complicated and I will walk through various scenarios and show impact on price 4 . First the formula and some definitions: \\begin{align} TP &= A+\\frac{MCR}{C}MCR\\%&#94;4 \\\\ \\text{where} & \\\\ TP &= \\text{Token Price in Ether} \\\\ A &= 0.01028 \\text{; This is a constant calibrated at launch} \\\\ MCR &= \\text{Value of Minimum Capital Requirement in Ether} \\\\ C &= 5,800,000 \\text{; This is a constant calibrated at launch} \\\\ MCR\\% &= \\text{Ratio of Capital Pool to Minimum Capital Requirement} \\\\ \\end{align} Note that the price of \\$NXM is inherently pegged to the price of \\$ETH and holding \\$NXM also has \\$ETH underlying exposure. As we can see, there are two main variables which decide the price: MCR and MCR%. MCR% is broken down into the numerator, capital pool, and denominator MCR. The capital pool is the total capital for Nexus Mutual to be able to pay out covers in case of black swan event and increases when people purchase \\$NXM. Before going further into the math of the bonding curve, I want to point out why \\$NXM is such an interesting token: it has a built in asymmetric risk reward opportunity. Entering at certain times, the downside risk is capped and the upside risk grows exponentially. I will explain the calculations and thought process that led me to this conclusion below. NXM Pricing Model: MCR The MCR is determined by the capital model 5 and is set to achieve a 99.5% probability of solvency over a 1 year period. MCR increases gradually as people are buying \\$NXM and it is set at a minimum of 7000 \\$ETH. MCR increases by 1% everyday that MCR% is greater than 130%. Note that MCR can only increase and never decrease and this feature intrinsically creates a base price for the \\$NXM token. \\(\\frac{MCR}{C}\\) is the long term portion of the formula. Because C is a constant and MCR can only go up, over the long run the tokenomics dictate that the price should slowly rise. If the price of \\$NXM goes down it is due to the short run portion of the model. NXM Pricing Model: MCR% MCR% is the short term portion of the formula and because MCR% is raised to the power of 4, small changes in MCR% can cause large ripple effects in the price. It is important to note the tokenomics in play when looking at MCR%. Whenever MCR increases, MCR% will decrease, because MCR is on the denominator. This decrease will also have a larger decrease in the price of NXM and will override the long term MCR base increase because of scaling factor. \\begin{align} MCR\\% &= \\frac{CP}{MCR}\\\\ \\text{where} & \\\\ MCR\\% &= \\text{Ratio of Capital Pool to Minimum Capital Requirement} \\\\ CP &= \\text{Capital Pool} \\\\ MCR &= \\text{Value of Minimum Capital Requirement in Ether} \\\\ \\end{align} NXM Pricing Model: MCR and MCR% Interaction If there is more capital flowing into Nexus Mutual than MCR increases, then MCR% will go up and \\$NXM price will also rise. The baseline model has MCR increasing by 1% every 24 hours so long as MCR% is larger than 130%. This means that capital must flow into the mutual more than 1% of MCR each 24 hours or the price of \\$NXM will go down. NXM Pricing Model: Restrictions There are a few restrictions in the pricing model: Capital Pool must be above the MCR, so MCR% cannot drop below 100. Redemptions are capped per transaction. Bid ask spread for \\$NXM is 2.5% on the \\$NXM exchange. Only members of mutual can own NXM tokens. Other addresses are whitelisted. NXM Pricing Model: Data I have created a Google Sheet to help better visualize the price of \\$NXM as MCR and MCR% change. In addition, here is a graph which sets the lines as the MCR and then shows price of \\$NXM in \\$ETH on the y-axis with the MCR% on the x-axis. We can see that the price increases exponentially as the MCR% changes and shifts up wards while keeping MCR constant. MCR only goes up and has linear movements, while MCR% has exponential movements on the upside, but can go up or down. To highlight the bonding curve formula more vividly, here is an example. Over the next year, if MCR% does not change, MCR will keep on increasing by 1% a day which will translate to \\$NXM price increasing more than 37x. If MCR% gradually falls to 130% and remains there, then \\$NXM price will increase by around 16x. If MCR% falls to 100% and stays there for a year, \\$NXM will drop by around a factor of 7. I believe in stacking asymmetric bets and \\$NXM presents one of the most asymmetric bets I have seen in crypto in a while. Alternative Data There are a few key performance metrics that exist to determine if the mutual is growing or shrinking and try to predict the future price of \\$NXM. These indicators include capital pool increases and amount staked. Capital Pool I mentioned this in the previous section, but it is worth repeating: capital must flow into the mutual at a rate greater than 1% of MCR each 24 hours or the price of \\$NXM will go down. Since the MCR is increasing daily as long as MCR% is above 130, everyday more capital must flow into mutual than previous day to keep MCR% the same and increase the price of \\$NXM. For example, if the MCR were to be 100k \\$ETH and the MCR% was over 130%, the capital pool would need to increase by around 1k every day to maintain the MCR%. I have tracked the capital pool data and plotted below. Since July, the capital pool has gone parabolic. Each day over 2500 \\$ETH has been added to the pool. Staking Any member of the mutual can stake their \\$NXM tokens on protocols to earn rewards. If there is an accepted claim on that protocol, the $NXM staked is burned and paid out. \\$NXM staking locks the tokens in the system for at least 90 days, which makes the tokens sticky and creates a baseline of tokens in the system. Currently there are around 140 million dollars staked and with a market cap of around 210 million, this means that around 66% of the \\$NXM tokens in circulation are locked and cannot be used to speculate in the short term. Increasing the amount staked, especially in proportion to the total amount of \\$NXM tokens is bullish for the price of \\$NXM as those cannot be swapped for \\$ETH. Protocol Change: Governance Vote Due to the explosion in DeFi, there ended up being more demand for cover than capital in Nexus Mutual, resulting in not enough capacity; because the capacity is limited by the MCR. Previously, MCR was bounded to at most increase by 1% per day if mutual has 'excess capital' of funds above 130% of MCR. On July 9th, the NXM community submitted a protocol change. Governance proposal 83 helped relieve the issue of a low MCR by reducing the MCR incremental time from 1 day to 4 hours necessary to increase MCR 1% if over the 130% MCR cap. This would revert back to once per day when the MCR reaches 100k ETH. This change allowed the mutual to attract capital faster and benefit from scaling faster. In essence, by looking at the pricing curve, we can see that this was a short term trade off for a long term gain. As long as MCR% is over 130, MCR would increase by 6% per day instead of 1% per day. By enacting the change, the MCR portion of the formula will increase 6 times faster, but made it very hard for the MCR% portion of the formula to increase. I have a chart of the MCR below and it is easy to see the MCR immediately spike from this protocol adjustment. At the current rate, Nexus Mutual will hit 100k \\$ETH MCR around August 21st and then the MCR will go back to only increasing once per day. Previously to maintain the MCR%, over 6k \\$ETH needed to be deposited daily. However, at 100k \\$ETH MCR, this is cut by a factor of 6 and only a little over 1k \\$ETH needs to be deposited daily to maintain the price. Anymore deposits would create an increase in MCR% pushing the price to a parabolic portion of the bonding curve. Just in the month of August there have been almost 8k \\$ETH deposited daily. Thus due to this protocol change, if the mutual can even maintain half as much deposits as earlier in August, the \\$NXM price will go exponential. Protocol Change: Investing It has been hinted at a few times on the website and throughout various sources, Nexus Mutual will invest its assets to generate additional yield for its members. There are many ways to generate additional profits with the large base capital, but this process is similar to how tradition insurance companies invest their funds. Trading There are two short term opportunities to make even more money in \\$NXM: arbitrage and front running. These are high conviction bets whose profits can be recycled back into the trades as there is decent volume. Arbitrage To buy \\$NXM, it is necessary to KYC on the Nexus Mutual website. Nexus Mutual then whitelists a wallet address where one can freely convert \\$NXM to \\$ETH and back. For those who do not want to \\$KYC, they can purchase \\$WNXM, which is a wrapped based around the \\$NXM token. Each \\$WNXM is redeemable here for a \\$NXM and the process can be reversed. The \\$WNXM token trades on multiple exchanges and any time these prices differ by a certain amount, people can make money bringing these prices back in line. Price differences will mostly occur when MCR% is high (more volatile prices) or everyday during the rebalance when the MCR changes. Frontrunning I hinted at this above, but the daily MCR change can be easily frontrun. The price of \\$NXM will temporarily decrease everyday at 14:00, which forces the price to go down. Take the scenario where we have MCR% at 131% and MCR at 100k \\$ETH, making capital pool 131k \\$ETH, and creating a price of 0.0611 \\$ETH from the bonding curve. At 14:00, the MCR changes to 101k \\$ETH and MCR% then drops to 129.7%, giving a price of .0596 \\$ETH from the bonding curve, a difference of almost 2.5%. Risks Putting in money in Nexus Mutual is not risk free. If there is a large payout event then the capital pool would get hammered causing a large drop in MCR%. In February, Nexus Mutual paid out its first claim. The current largest insurance claims are for the Curve stablecoin and BTC pools (25k \\$ETH) and Yearn (20k \\$ETH). Paying out these covers would cripple \\$NXM price in the short term and would see MCR% ranging from 100% to 130%. Finally, current MCR% is high and the risk-reward tradeoff would be a lot higher to buy NXM when the MCR% is closer to the 130% mark. Conclusions \\$NXM presents a great opportunity to invest in a company with clear use-case for the growing DeFi landscape. In order to stake the tokens, I would need to be more familiar with the code of these protocols, but \\$NXM stands out as a clear winner in the space whose price will appreciate as DeFi grows. I have accumulated a position of \\$NXM to hold while also taking advantage of the tokenomics to trade in and out of shorter term positions. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); } This chart is slightly misleading as most crypto assets have increased substantially this month so the dollar value would appreciate regardless. Looking at this chart in terms of \\$BTC or \\$ETH value is a more accurate representation of asset growth. ↩ There often is an issue with double counting in DeFi, as some coins can be used by multiple protocols. ↩ Will need to KYC to purchase \\$NXM. One can also buy \\$WNXM for exposure to the \\$NXM price movements. ↩ The \\$NXM price is currently pegged to the price of \\$ETH. ↩ The actual formula is created by the European Insurance and Occupational Pensions Authority with two components, a best estimate liability which is the expected loss on each contract cover, and a buffer for a black swan event. ↩","tags":"Crypto","url":"nexus-mutual-insurance.html","loc":"nexus-mutual-insurance.html"},{"title":"The Exchange Pump: Coin Tracking (Part 3)","text":"Recap Previously I wrote two articles, one describing how listing on tier 1 exchanges is beneficial to price of coin, and the other detailing how to identify these potential listings. This article focuses on trying to pinpoint when an exchange will list a particular coin. Etherscan Etherscan is a website which allows people to track transactions on the ethereum blockchain. Specifically, people can search specific transaction ids to see the wallet that sent the tokens, the wallet that received the tokens, how much tokens were sent, and when the transaction occurred. Another tool on etherscan is the ability search by wallet id to see how much tokens are in a wallet and the historical transactions of all the tokens in said wallet. This is akin to a continuous time 13-F form that traditional finance uses. At the end of every quarter, funds with at least \\$100 million assets under management must share their holdings. Often times analysts study these hedge fund positions to try to capture some of that supposed alpha. Due to the nature of the open blockchain, I am similarly able to query the current holdings all the players in the ecosystem. However, this is even more powerful than a 13-F, because the blockchain allows me to see all the holdings live rather than the once per quarter snapshot. Trade Theory Delving one step deeper, if I know the exchange wallet ids, I can see all transactions and holdings of said exchange. This can be used to find new listings, because I can constantly scan an exchange wallet to look for coins deposited in the wallet that are not listed. Prior to listing, an exchange would need to store some coins and test the deposit/withdraw mechanisms as well as trading logic dedicated to the new token listing for bugs. So if I see a large increase of a unlisted coin in the wallet address of a large exchange, it is a very probable bet that the exchange will list the token soon. Binance Example A few days ago, on August 10th, Binance listed $YFI 1 also known as yearn finance at around 09:28 UTC. Just like the other Coinbase coins in my previous blog post, \\$YFI immediately rallied after this announcement, going from \\$4200 to \\$6500, before settling down around \\$6000. However, looking closely at the Binance wallet on Etherscan, the first \\$YFI transaction in Binance occurred at 4:34 UTC, almost four hours before the official blog post and tweet. In fact there were over 50 \\$YFI transactions with the Binance wallet before the official announcement. If someone where to been tracking popular unlisted coins on the ethereum blockchain, they would have been able to frontrun Binance and the exchange listing. In fact there is a small run up in the price and volume of \\$YFI before the accouncement and I believe some other savvy participants noticed these Binance transactions and started to buy \\$YFI. Conclusion I can use information from alternative data to search for which coins I believe will list and then if these coins are on the ethereum blockchain, I am able to predict when they will list. I view these trades as asymmetric bets, because if the exchange does not list the token, I can get rid of the coin close to buy price. On the off chance that the coin is listed, the returns are usually astronomical. It would be interesting to get statistics on how often each exchange has transactions from non-listed coins and how often these become actual listings. This coin has very strong fundamentals and I am very bullish on it. Will make a seperate post on the fundamentals. ↩","tags":"Crypto","url":"exchange-pump-3.html","loc":"exchange-pump-3.html"},{"title":"The Exchange Pump: Aligned Incentives (Part 2)","text":"Aligned Incentives In the prior article, I explored the impact of a new listing on a tier one exchange and showed that listings are very beneficial to a coins price in the near term. However there are thousands of coins, so its very difficult find which one will be listed and guessing specific coins is a low probability endeavor. In this article, I will show how it is possible to find coins with high probability of listing by looking at alternative data. Exchange Investments In traditional finance, the Volcker Rule was created to remove banks from investing as this was deemed a conflict of interest with the customers they served. Thus many internal trading desks and investment arms of these banks were wound down and spun off. In the crypto world, these exchanges hold a vast amount of power as they act as a bank, an exchange, a broker, as well as a hedge fund. In a clear conflict of interest, these top exchanges all have venture arms where they invest in coins and then are able to list the coins on to their own exchanges to give these coins a nice pump. While this action is shady, it can be taken advantage of as I know that the exchanges are highly incentivized to list their own coins. Thus, by examining the holdings of the venture portfolios of the top exchanges, I can narrow down the coins, and have a high conviction in the specific coins that will be listed. Huobi Capital Below is a picture of all of Huobi Capital's (the venture arm of Huobi exchange) investments . WalletCurve does not seem to have a coin and I cannot find any information on Buxx, however Huobi has listed the 5 of the other projects: Internet of Services, \\$IOST Ren, \\$REN Nervos Network, \\$CKB Terra, \\$LUNA ThunderCore, \\$TT The only one that Huobi has not listed is dForce, probably because it was a recent investment, but I have high conviction that it will be listed eventually. Other Exchanges Both of Binance Labs as well as Coinbase Ventures have large portfolios that contain a variety of companies some of which have launched coins. There are a few that have not been listed and I expect them to be listed soon. In no particular order they are \\$UMA, \\$RSR, \\$MATIC, and \\$LUNA. Popular Coins Exchanges make money off trading volume. If a lot of people want to trade a token, it is in the exchanges best interest to list said token so they can collect trading fees. Trading fees are inherently a zero sum game, because if I trade on a certain exchange, that exchange will get the trading fees and other exchanges wont. So when exchanges see large volume from a coin, they also rush to list said coin to get a piece of the pie. $YFI launched on July 18th and has consistently had tremendous amounts of volume on decentralized exchanges 1 always ranking in the top 5 in volume traded. Below are the exact volumes from launch date till today. The dash red line is August 10th, when Binance listed \\$YFI. \\$YFI averaged around 50 million dollars in daily volume primarily on decentralized platforms like Uniswap and Balancer. Looking at the volume charts, this made \\$YFI a top 50 coin 2 based on volume traded. Binance has listed almost all of the coins higher on the ranking and adding \\$YFI was very low hanging fruit. Fast forward to today and Binance has captured around 75% of the \\%YFI volume. Exchange Profits Here is a quick calculation on how much Binance makes in profit off of trading fees. The numbers may be slightly off, but in the Binance whitepaper, 20% of all exchange profits go towards \\$BNB token buybacks. In the year end review blog we can see that around \\$207,338,000 worth of \\$BNB was burned. This means that the total profit of Binance was around 5 times that amount at over \\$1,000,000,000. Looking at the average trading volume, we can see that there are \\$2,852,591,354 dollars in trades per day. Multiplying by 365 gives us around 1 trillion dollars in traded volume. Thus with a easy back of the hand estimate, we can see that Binance makes approximately 10 basis points per dollar traded, in line with their fee schedule . 3 In the 4 days after listing, Binance traded around 37.5 million dollars per day of \\$YFI. This equates to around 37.5k in profits for Binance. If those numbers stayed constant for the entire year, Binance would make around 3.5 million from the \\$YFI listing. Listing a particular coin is very low hangning fruit as Binance has over a hundred different ones and optimized the procedure. It is pretty clear why Binance and other exchanges are heavily incentized to list these high volume coins. Conclusion By looking at the revenue model of the exchanges, we can narrow down the list of new addition candidates substantially. These exchanges have no KYC and will list anything. A negative side-effect is that there are alot of scam tokens. ↩ Stable coins are not counted in this volume assessment ↩ The calculations are slightly off as Binance does other things that are also profitable, but fees are the primary driver and this is just a ballpark number ↩","tags":"Crypto","url":"exchange-pump-2.html","loc":"exchange-pump-2.html"},{"title":"The Exchange Pump: Introduction (Part 1)","text":"Tier 1 Crypto Exchanges When a coin lists on tier 1 exchanges, the coin often receives a price increase. A tier 1 exchange can loosely be defined as an exchange that is secure, has good liquidity, and is trusted by the community. Two of the most popular coin price aggregator sites Coingecko and CoinMarketCap have their respective exchange rankings, and most people agree on some order of Binance, Coinbase, Huobi, Bitstamp, and Kraken. Of these tier 1 exchanges, Binance and Coinbase have the most new coin offerings. Listing Theory I am not sure why the coins listed on these exchanges recieve this immediate price increase, but my two working hypothesis are: These exchanges are deemed trustworthy so the coins they list are likewise given a trustworthy stamp of approval, which increases the value of the coin. Some market participants may only have accounts on these top exchanges and thus were unable to purchase these coins before when they were available for presale or on smaller exchanges. These top exchanges are very popular fiat on-ramps 1 and a lot of more novice users can deposit their respective home country currencies and buy crypto. So when a tier 1 exchange lists, there are more participants 2 similar to an IPO. Numearie Example Yesterday Coinbase announced the listing of Numearie, \\$NMR, through its twitter account and on its blog. Below is a picture of the \\$NMR price before and after the tweet. We can see a rapid increase from \\$20 to around \\$60, before stabilizing at around \\$50, over a 100% increase just due to the Coinbase listing action. Historical Examples Focusing on the Coinbase exchange which has some of the most retail flow, I will examine the performance of all of the newly listed coins 3 before and after the Coinbase announcement. Because the listing announcements span both across bull and bear markets, it is more reasonable to look at performance in BTC terms as opposed to dollar terms. Data Data is pulled from the Coingecko api . Some coins like \\$OXT and \\$COMP are excluded, because Coingecko did not have sufficient clean data prior to announcement. I also excluded stable coins like \\$DAI as they should be inherently pegged to the dollar and removed \\$NMR, because there is not enough data. This leaves us with 22 new listings to examine. Below we see a group of plots, where the x-axis time is in days and time=0 is the listing date, 1 is be the day after listing and -1 is be the day before listing. The y-axis shows the cumulative return from time t=-2 to time t=2. We can see that there are clear outliers like \\$LINK and \\$BAND which increased over 50% on listing day, but most of the time there is still a clear positive (albeit smaller) gain from listing. I have included the mean and median of these cumulative returns and we can see that on average there is around a 15% increase from listing and the median listing increase is approximately 8%. Next Steps In the upcoming days, I will write two follow up articles about strategies that can be used in tandem to front-run these exchange listings. The first will be how to identify coins which have a high probability of listing and the second will be how to guestimate when the coins will be listing. They have good relationships with regulators and banks of various countries in addition to strong marketing teams. ↩ It can also be argued that these new participants are less crypto savvy and filled with more 'retail' traders. ↩ I do not consider Bitcon Cash a new listing. ↩","tags":"Crypto","url":"exchange-pump-1.html","loc":"exchange-pump-1.html"},{"title":"Cryptocurrency History: The Birth of Bitcoin","text":"Background In late 2008, the global financial crisis was in full swing and multiple banks were on the verge of insolvency. Lehman Brothers filed for bankruptcy and it took a historic government bailout to rescue the other financial institutions. In the midst of this distrust for banks and governments, on October 31, 2008 Satoshi Nakamoto released the bitcoin whitepaper . Most people in the crypto universe would consider this event to be the start of cryptocurrencies. This post I will walk through the whitepaper and delve deeper into each section. There are a total of 12 sections: Introduction Transactions Timestamp Server Proof-of-Work Network Incentive Reclaiming Disk Space Simplified Payment Verification Combining and Splitting Value Privacy Calculations Conclusion Introduction This first paragraph explains the current flaws in the electronic payments system. As it stands, we are in a trust based system, where banks and other financial intermediaries are third parties who process payments and arbitrate disputes. These disputes inherently cause transactions to be reversible which creates some issues for all of the participants in a transaction, the consumer, the merchant, and the intermediaries. Intermediaries have vast power. Assuming the intermediaries are fair, this still gives them unchecked power as they are the judge, jury, and executioner of every dispute. There is a reason why the legal system is split to the point where multiple people performing the roles above. The consumer must share their data to both the intermediaries and merchants. The merchant bears the fraud risk and the intermediary takes on the consumer credit risk. To counter this, both the merchant and the intermediary will collect financial information on the customer (salary, assets, etc.) in order to asses the consumer. The consumer and merchant must pay for the additional services of the intermediary. Along with the fee to process the payment, there is a built in fee to mediate disputes. Currently credit card companies charge merchants approximately a 2% fee of every transaction for their many services. Satoshi Nakamoto purposes an alternative payments system based on cryptographic proof instead of trust in a third party. The cryptographic proof system would revolve around irreversible transactions, which would remove the financial intermediaries, lowering the transaction costs, while still protecting the merchants from fraud. Buyers could also be protected by escrow mechanisms. A brief description of the new payments system is that decentralized computers verify the irreversible transactions, creating a electronic receipt (the blockchain) that all parties can view. Transactions Nakamoto starts off this section with the definition of an electronic coin as a chain of digital signatures, known more commonly as the blockchain. A blockchain is a decentralized database that stores digital transactions, but in accordance with the irreversible transactions, it only add records when transactions occur and never edits or deletes them. The main issue with this is procedure is the double-spend problem: how can parties verify ownership and prove people are not submitted duplicate payments to multiple participants? In the current centralized electronic system, banks have access to both buyer and seller and thus can correctly credit or debit accounts respectively. In the cash based system this is not an issue either as people hand over cash to each other and cannot double spend, but in a decentralized system the double-spend problem is a major dilemma. Nakamoto solves this by publicly announcing all transactions on the blockchain so all parties are aware of a coins history. The next step would be for the parties to agree on a single transaction history, which then assigns coins to each participant. To do this, the blockchain needs to record timestamps for each transaction. Timestamp Server The timestamp server is the software that timestamps each transaction by taking a section of the transaction data and adding timestamps to it to create a hash. This hash is a complex algorithm (SHA-256) that miners solve before the transaction can be added to the blockchain. Note that this fixes the double-spend problem by adding a time priority to the transactions. If same coin is sent to multiple parties, only first transaction is the official one that all parties agree to. Proof of Work Because of the decentralized nature of the blockchain, multiple transactions can be created at the same time and could arrive at the network at the same time. This in turn clogs the network and prevents validation of these transactions. Proof of work, also called bitcoin mining, is the action that validates the transactions on the blockchain and then adds on the new block to the other validated chain of block. Proof of work involves the miners solving complex math equations that are hard to solve but easy to verify. 1 To be specific, computers scan for a particular value which when hashed begins with a certain amount of zeros. At this point, the newly created block adds reference to data from the previous block. It is important to note that this causes the block information to be path dependent. If someone wanted to change something on this particular block, they would need to edit all previous blocks. In addition, this means that older blocks are harder to change than newer ones. Nakamoto also mentions that the difficulty to mine a bitcoin should be a moving average to target a fixed amount of blocks per hour. If blocks are generated too fast, the difficulty increases and vice versa. Network Nakamoto lists six steps to run a node (mine bitcoin). New transactions are broadcast to all nodes. Each node collects new transactions into a block. Each node works on finding a difficult proof-of-work for its block. When a node finds a proof-of-work, it broadcasts the block to all nodes. Nodes accept the block only if all transactions in it are valid and not already spent. Nodes express their acceptance of the block by working on creating the next block in the chain, using the hash of the accepted block as the previous hash. It is worth reiterating that nodes always consider the longest chain to be the correct one and will work on extending it. There are times in which two competing nodes create two different branches of the blockchain, but eventually one branch will have more blocks and become the 'correct' version. Incentive Bitcoin mining is expensive and the miners need to be rewarded for their service. After a block has been successfully mined, the block miner recieved bitcoin. Similar to gold miners who use labor and oil resources to find gold, bitcoin minters use CPU/GPU and electricity to create bitcoins. This process is beneficial because it rewards bitcoins to those who support the network. 2 It serves a further purpose as a way to bring the coins into circulation as these is no central bank to print money. There are also transaction fees paid to miners which are more incentives to keep the network running smoothly. From another point of view, the rewards should encourage nodes to be honest. If a dishonest miner is able to overtake the CPU power of the honest miners, he could overwrite the existing blockchain or generate new coins. The incentives should make it more profitable to support the network and generate more coins through mining then trying to reverse transactions. Reclaiming Disk Space Nakamoto lists potential ways to save memory space, because chains get longer and longer as time goes on. He suggests a MerkleTree to compress the data by only storing the root, however also mentions that Moore's Law and the increasing of RAM could make this a moot point. Simplified Payment Verification It is possible to take a shortcut and send, receive, and verify blockchain transactions without having a full node. Nakamoto suggests a shortcut to copy a few block headers of longest proof-of-work chain, obtain the Merkle branch for transaction to block and seeing if a network node has accepted it. Combining and Splitting Value Transactions to same recipient can be combined for more efficient transfers. Privacy Everyone will be able to see the transactions, but if the keys are anonymous, then it is hard to track. In addition the information is encrypted so only the holder of the private key can read and understand the messages. Nakamoto uses the stock market as an example, where the entire market knows the trade timestamp and price, but not who bought or sold the stock. He also suggests users to create a new key for each transaction as an additional method of security. Calculations What is the probability that a attack on the blockchain from a dishonest miner succeeds? It turns out that this would create a race between honest chain and attacker chain, which can be modelled like a binomial random walk. A success is honest chain extending lead by one block, increasing lead by +1, while a failure is attacker chain extending by one block, reducing lead by -1. This is the gambler's ruin problem presented below: \\begin{align} q_z &=\\left \\{\\begin{matrix} 1 & \\text{ if p$\\leq$q}\\\\ (q/p)&#94;z & \\text{ if p$>$q} \\end{matrix}\\right. \\\\ \\text{where} & \\\\ p &= \\text{Probability an honest node finds the next block} \\\\ q &= \\text{Probability the attacker finds the next block} \\\\ q_z &= \\text{Probability the attacker will ever catch up from z blocks behind} \\\\ \\end{align} The more $p>q$, the less of a chance the attacker has to catch up. Given that the function is raised to the power of $z$, if there is no lucky few breaks, the probability that the attacking chain wins exponentially declines. Conclusion The main issue with the current payments system is that it relies too much on the blind faith of financial institutions. A decentralized approach based on cryptographic proof removes the need for an intermediary, however it creates other issues like the double spend problem. The blockchain solves the double spend problem by creating a proof-of-work record with a public history of transactions. In addition, protocols are built in to make reversing transactions computationally impractical. Rewards are distributed to miners to facilitate the transactions. The network is robust in its unstructured simplicity. This point is one of the most important tenets of the blockchain. ↩ There will only ever been 21 million bitcoins giving bitcoin deflationary characteristics. ↩","tags":"Crypto","url":"bitcoin-whitepaper.html","loc":"bitcoin-whitepaper.html"},{"title":"Counting Stats","text":"Counting stats, otherwise known as basic stats, have long dominated basketball discussion, because they are easy to understand and the data is easy to collect. Anyone can look at a boxscore and analyze which players had good games or bad games; the issue is that these counting stats mask the process of the ball. They only show the end result and not the process (Embiid shout-out) of how the result was created. Take an example from finance -- below are two theoretical equity returns of two assets which show the percent return from time t=0 to time t=100. Both of these assets return 100% in 100 periods, but they have very different paths to the 100% return. Path 1 is very straight and linear, returning 1% every day for 100 days, while path 2 has a lot more volatility and drawdowns. To an investor who does not care about the paths that the assets take and only the end result/return, the assets serve the same purpose: they return the same net amount. This type of investor is only interested in the asset price at t=0 when he buys it and the asset price at t=100, when he sells it and cashes out. All the information this investor needs can be captured with the two data points t=0 and t=100. This type of investor is similar to a person analyzing the box score, because the boxscore only contains the end result, not what happened in between to achieve that result. However, if a person was to go more granular into the asset returns and look at the day by day paths he would see that the two assets are not the same. I bring this finance example into discussion, because not all counting stats are the same and equal, but they are often discussed as such. By looking at the boxscore, one is able to see certain offensive categories like points, threes, rebounds, assists and defensive categories like blocks and steals. Below is the Warriors boxscore from the Raptors-Warriors game on 2016-11-16. In this boxscore, we can see that Klay Thompson took 11 threes and made 3 while his backcourt mate Steph Curry took 9 and made 3. Suppose that Klay Thompson also took 9 threes and still made 3 of them for the same shooting percentage as Steph Curry. Then it would take a small jump of logic to say that Klay and Steph both shot 33% and had similar shooting nights. This small jump of logic is similar to the fallacy in the two financial assets, because Steph Curry and Klay Thompson create their threes in very different ways: Klay Thompson is a lot more screen dependent then Steph Curry is. When Klay Thompson gets open and shoots a three, often times there are many screeners setting multiple screens to free him up. If Zaza Pachulia sets Klay Thompson a perfect screen that frees him up for a wide open shot and Klay Thompson hits the shot, Thompson gets all the credit in the boxscore. The boxscore will increase Thomspon's FGA by 1 and his FGM by 1. Zaza Pachulia gets no credit. But when Zaza Pachulia gets called for an illegal screen by attempting to free Klay Thompson up, Pachulia gets a TO credited to his box score, while Thompson gets nothing. So Thompson is getting all of the benefits of a Pachulia screen with none of the disadvantages in the boxscore, because the boxscore only focuses on the final result of a make or a miss and not the process. An accurate statistic should incorporate the fact that Pachulia also helped in creating the shot for Klay Thompson and award him appropriately when Thompson hits a shot, while punishing Thompson when Pachulia sets an illegal screen. More specifically, tracking data can calculate how close the nearest defender is to Klay Thompson when he shoots and give/take credit from Zaza based on this distance. I bring up Thompsons's backcourt mate Steph Curry, because he shoots many of his three pointers off the bounce which do not require a screener. And because he creates more of his shots from a 1-on-1 perspective, his style is very different than Klay's, like how the two assets have different paths but all lead to the same net outcome. Curry's 1-on-1 playing style (although he does have off ball screens and pick and rolls) creates a more accurate representation of his 3-point shooting percentage when looking at the counting stats of 3PA and 3PM, because the screener is not being accounted for. Klay Thompson shooting around 42% (a very elite number) from three during his entire career, however the counting stats have inflated his number. Suppose that Zaza Pachulia (and previously Andrew Bogut) sets an illegal screen that gets called 10% of the time resulting in a turnover. Instead of hitting 42 threes in 100 posessions, Thompson would instead hit around 38 threes in 95 attempts (but 100 possessions) giving him a real shooting percentage of 38%. This are all made up numbers and a moving screen should not be 100% the shooters or the screeners fault, but this example is just an illustration of how counting stats, while they give a good summary, are not entirely accurate in the picture they paint. Summary: Counting stats paint a slightly different picture than what goes on in a game. Only looking at counting stats neglects the process of the game, just like in finance how only looking at the end result removes intermediary fluctuations. Counting stats create a snapshot in time and summarize an entire possession of time in that snapshot. If Klay Thompson spends 20 seconds running around 2 picks and an elevator screen to get opened, there are multiple people contributing to the final result, however all that gets recorded is what Klay Thompson does. Similarly, a rebound records the player coming down with the ball, but what about his teammates boxing out the opposing players? Shouldn't they get a little credit too?","tags":"NBA","url":"counting-stats.html","loc":"counting-stats.html"},{"title":"An Analysis into Gold","text":"Simple Gold Fair Value Model A quick and dirty way to price gold is to look at the total amount of money in circulation (M2) and divide it by the size of gold stock. This would quite literally tell how much gold is worth. It is kind of difficult to precisely measure all of the gold stocks in the world, but there are rough estimates we can use. Splitting gold into jewellery, private investment, official holdings, and other, we see around 197.6k tonnes in existence at the end of 2019. In addition, we know that mining adds around 2.5-3k tonnes annually. Given that we are midway through 2020, I will add 1.5 tonnes and assume that we have a total of 199.1k tonnes of gold in existence. Turning this into troy ounces, we get around 6.4 billion troy ounces of gold. As of July 16th 2020, M2 money stock was 18.522 trillion giving gold a fair value price of 2894 per troy ounce and at the current gold price of around 1870 there is roughly a 35% premium. Unfortunately, I cannot get a time series of global gold stock to see compare the price of gold versus this fundamental calculation. However, I will try to estimate this number since the removal of the Bretton Woods agreement when the world was taken off the gold standard by looking at the amount of gold produced annually 1 and subtracting it from the 197.6k tonnes at the end of 2019. Using the same formula as above, I calculate the theoretical price of Gold and plot it against the traded price. We can see that gold typically trades at a large discount, but there are periods that it catches up or even surpasses the fundamental price. So why am I so bullish on gold right now? Below is a plot of M2 for the past 5 years. The line grew very steadily with a constant slope until March 2020, when the Fed relaxed lending rules ( I , II , III , and IV ) and M2 took off like a rocket. Now gold is knocking at all time highs (in dollar terms) and I do not see the Fed reversing their policies anytime soon. The tailwinds are in-place for another gold run. A side note is that other central banks like the ECB are also engaging in similar techniques increasing the global money supply. Further more the stimulus checks that the government hands out are were delivered via direct deposit into the checking account for around 90 million Americans. The same will be true for the purposed second round of stimulus checks and both of these stimulus checks are direct increases of money in circulation. Other Factors on Gold Price The above model is a pretty rough estimate which only looks at two factors, money in circulation and gold stock. Some of the other main drivers are below and I will examine a few of these in detail. Gold price moves inversely to real interest rates. Gold is correlated to central bank balance sheet expansion. Like any other asset gold price is related to demand and supply. Gold is a safe haven asset and will rise in periods of global uncertainty. Gold is correlated to other commodities and will rise and fall in tandem. Interest Rate One of the strongest predictive and contemporaneous metrics to gold price is real interest rates. When real rates (interest rate minus inflation) are low or negative, the real return of bonds are unattractive as an asset class and people look for alternatives. Claude Erb and Campbell Harvey look at the relationship between real interest rates and gold price and see a -.82 correlation between the two. We can use the 10-year TIPs (Treasure Inflation-Index Securities) to proxy the real interest rates and see how the relationship has performed in the past few years. From 2003 to present day there is a -.88 correlation and below is a plot of the real rates and gold price. We see that other than a few liquidity events which impacted the price of gold but not real rates, there is a clear relationship inverse relationship between the two. Another way to visualize the strength of this relationship is to just multiply the real rates by -1, and look at the graph of inverse real rates to gold price (also plotted below). Even before COVID-19 we had negative real rates and COVID-19 just accelerated the decline to levels not seen in a few decades. Until there is a cure for COVID-19, I do not see how this trend is going to be reversed and even with a cure, some economists are projecting years until we get back to pre-corona virus levels of economic activity. Central Bank Balance Sheet Aggregating some of the largest central banks in the world (Fed, ECB, BoJ) 2 , I created a total balance sheet of these central banks. Then I take rolling 12 month differences between this aggregated global bank balance sheet and the price of gold. I am able to see that gold typically appreciates in value when the Central Banks expand their balance sheet. The major exception to this case would be in 2008, when gold tanked, but that was more due to a liquidity crisis where people were selling anything to get cash. In 2020, the Fed is taking unprecedented action to inject liquidity into the market by buying corporate bond ETFs. While I expect the expansion of the balance sheet to slow down, there is no indication that the Fed is planning to shrink the balance sheet anytime soon and this indicator is bullish. Central Bank Gold Reserves One of the main drivers of gold demand is central banks. We can see that central banks have been net buyers of gold. There are around 2500 to 3000 tonnes of gold mined annually and central banks are buying around 15-20% of the new gold and adding it to their reserves. Below we can see that cumulative central bank reserves have been increasing by around 1000 tonnes every 2-3 years or the past 12 years. This trend should continue into the foreseeable future and some central banks (Russia and China) have even talked about increasing reserve purchase amounts. ETF Flows A good proxy for gold supply and demand are the ETFs as they are a lot more liquid and accessible for the everyday investor. A lot of these ETFs charge high expense ratios, because they store the physical gold. From gold.org , we can see the historical ETF flows around the world from 2003 till current day. I want to note the strong correlation between gold price on the right hand side and ETF flows. Positive ETF flows typically create positive gold prices and vice versa. Looking at this chart, only one year has there been positive ETF flows and a drop in gold price. The second point I would like to note is the historic amount of ETF inflows 2020 has seen. We are only halfway done the year, and already there is more than 500 tonnes of gold ETF inflows, the most in any year so far. I fully expect more gold inflows and there is a lot of price reflexivity here. As gold price increases, more people get bullish on gold and this creates an feedback loop. It has been 9 years, but gold is finally nearing all time highs in dollar terms. Conculsion Many of the historical gold indicators are flashing buy signs at the same time. I believe that gold is poised for a continued breakout past its all time highs. In the next few posts, I will talk about other some other assets that will rise in tandem with gold and some other investment possibilities. The gold production data was collected from the United States Geological Survey, which has annual statistics on a variety of minerals. ↩ PBoC is the other large central bank, but I was unable to find up to date data on its holdings. ↩","tags":"Investing","url":"metals-1.html","loc":"metals-1.html"},{"title":"Dell VMWare split","text":"Dell Technologies, \\$DELL, owns 81% of VMware, \\$VMW, through its \\$67 billion purchase of EMC. Looking at the prices at the close of 2020-07-16, we see that VMWare is worth 58.61B while Dell is worth 43.78B. This means that the portion of stock in VMWare Dell owns is worth 47.5B. According to the market, Dell without VMware is worth around negative 4B. It seems that a spin-off is in play and would be beneficial to both companies. Dell bought VMW in September of 2016 so he needs to wait 5 years till 2021 and there will be a spinoff in a tax-free transaction fashion. For what its worth, the closest competitor to \\$DELL, \\$HPE trades at a similar PE ratio to \\$DELL, has similar margins in the PC space and is worth 12B. In 2013, Michael Dell and Silver Lake Partners bought out Dell's shares in the public market and took Dell Technologies private. 5 years later, Dell Technologies returned to the public makets by buying back shares that tracked the financial performance of VMware. Using historical market caps (from ycharts), I have plotted a time series graph of \\$DELL marketcap without the \\$VMW portion. The exact forumla is below \\begin{align} D_{\\text{val}} &= DELL_{\\text{Market Cap}}-.81*VMW_{\\text{Market Cap}} \\\\ \\text{where} &= \\\\ D_{\\text{val}} &= \\text{Market Cap of Dell Technologies without VMWare} \\\\ DELL_{\\text{Market Cap}} &= \\text{Current Market Cap of Dell Technologies} \\\\ VMW_{\\text{Market Cap}} &= \\text{Current Market Cap of VMWare} \\\\ \\end{align} We can see that the intrinsic value of $D_{\\text{val}}$ started at around -10 billion when it listed and has hovered mainly between -20 billion and -10 billion. It is currentely at an all time high of -4 billion due to \\$VMW spinoff rumors. I believe that the fair value of \\$DELL would be in the ballpark of \\$HPE given that they are competitors and \\$DELL has higher gross profit and profit margin along with a similar PE ratio. Even if $DELL is only worth 10B, thats still a 15B increase on the current -4B price and almost a 35% increase on the current market cap. Potential Trades I think there is the obvious trade which is to long \\$DELL and then there are two hedged plays which may be more lucrative from a risk-reward standpoint. The first hegdged trade would be to long \\$DELL and short \\$HPE, which is betting on both \\$VMW increasing and \\$DELL increasing from the -4 billion valuation. A more involved play would be to short both \\$HPE and \\$VMW and only bet on Dell's -4 billion valuation increasing to something reasonable. For what its worth I am long \\$DELL, because I believe that they will spin-off the VMWare shares and I wish to be long \\$VMW as I think it is a very cheap business with highly profitable margins in the SaaS space. I will write a followups for why I like \\$VWM and track how the proposed trades suggested have done.","tags":"Investing","url":"dell-vmware.html","loc":"dell-vmware.html"},{"title":"Ampleforth","text":"In the paper Synthetic Commodity Money , George Selgin describes Bitcoin and other cryptocurrencies as synthetic commodities that address shortcomings traditional stores of value have. Specifically fiat money has mismanagement issues (queue present day Federal Reserve money printer) and commodities have unexpected supply shocks. The issue with Bitcoin is that there is a fixed amount, so deflation is built in. Another cryptocurrency with more elastic supply would have the benefits Bitcoin has as well as fix the deflationary issue. Ampleforth tries to tackle this exact issue. Ampleforth, \\$AMPL, is one of the most interesting coins, because the supply of it changes daily based on the price. When the price is high, the wallet balances increases and when it is low the balance decreases. The main purpose of this rebase is to dampen the volatility of the price, giving it an intrinsic ability to be a stablecoin and price other assets off of Ampleforth. In essence, the larger the market cap, the more stable \\$AMPL price will be. The whitepaper can be found here . How the rebase works works I will outline two specific scenarios. Jordan buys 1 Ample for $1 and the price soon increases to $2. The price-supply equilibrium would 'split' the Amples and Jordan would end up with 2 Amples worth $1 each. The total net worth of Amples is constant, this is literally a stock split. Say after this Jordan's Amples shoot down to $.5. Similarly, after the rebase period, he will end up with 1 Ample worth $1. This is akin to a reverse stock split The exact details are subject to change but currently if the price is above a target threshold the supply expands and if it is below the supply contracts. Specifically the current price target is one 2019 US dollar and price threshold is at 5%. The rebalance occurs gradually over 10 days to smooth out the supply changes, but the key here is that this process is memoryless. The previous days supply change does not matter when calculating the current days supply change. Whatever the supply should be is just adjusted by \\(\\frac{1}{10}\\) everyday. More details can be found in this article written by the founder. Trading Implications This rebalance feature has massive implications for the trading world. It can be very harmful to leave stale bids as they can easily get swept from traders gaming the rebase period which occurs daily at 02:00 UTC. Also it should be possible to predict the rebase direction and benefit from front-running this equilibrium point. Investing Implications Unlike most crypto coins, the price of \\$AMPL is really secondary to the market cap due to the rebase protocol. Below is a market cap and price over the past month from coinmarketcap . We can see that while the price increased from $1 to around $2.3, around a 1.3x increase. However the market cap during this same period increased from 6 million to 120 million, a 20x increase and the true value of the investment during this period. Additional Thoughts Due to the rebase mechanism, \\$AMPL should have very unique returns characteristics, uncorrelated with other assets both in the traditional space and the crypto space. I believe that it is the ultimate momentum asset because as long as the price stays above $1 the supply will increase. People will see that their net worth has gone up and buy more \\$AMPL. Thoertically people will sell the extra \\$AMPL for every new issuance, but I do not think that will happen. People will chase returns and put even more money into \\$AMPL to recieve a larger split. In a market prone to rapid bull runs, it is against human nature to sell. They will infact buy more \\$AMPL, leading to an increase in \\$AMPL supply and potentially causing a large feedback loop. I am not advocating for the fundamentals of \\$AMPL, rather I believe that the built in mechanics give it a shot to multiply a lot more than some of the other smallcap crypto coins. I believe return distribution should have a lot higher right skew and kurtosis than a normal smallcap. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; var configscript = document.createElement('script'); configscript.type = 'text/x-mathjax-config'; configscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" availableFonts: ['STIX', 'TeX'],\" + \" preferredFont: 'STIX',\" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript); (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Crypto","url":"ampl-intro.html","loc":"ampl-intro.html"},{"title":"Crypto Intoduction","text":"I have been around crypto since the days of SilkRoad and Mt. Gox. I still remember my college friend buying Bitcoin for under $20 and then ordering edibles off of SilkRoad. Living in a dorm there was free electricity so my suite-mate started to mine Litecoin. That was the early adaptor phase, but after the crash of Mt. Gox cryptocurrencies faded from the mainstream media until the ICO bubbles in 2017. I was working at a trading firm at the time, and the senior traders likened it to the 2000's tech bubble. Anything that you would buy would go up. It was also around this time that the crypto exchanges were very inefficient and prices were not in line at all and I would buy and sell on various exchanges to try to arbitrage the prices, mainly taking on exchange risk. And some of these exchanges were definitely scams like Bitgrail and Cryptopia. By mid 2018, the bubble had burst and crypto was once again headed to the 'dark ages'. A good way to look at the state of the crypto market is a graph of bitcoin dominance, which is the total market cap of bitcoin over the total market cap of crypto assets. Typically during bitcoin bear markets, bitcoin dominance will be higher, as the altcoins have a much larger beta than bitcoin does. We are currently in the middle of a bitcoin winter, with bitcoin stuck between 9-10k, but I am very bullish on the long term possibilities. Take that with a grain of salt as I have been bullish ever since I heard about Bitcoin. Honestly speaking, there are a lot of interesting projects being built right now especially in the DeFi space. I will shed light on interesting projects and look for potential investments in my upcoming blog posts.","tags":"Crypto","url":"crypto-intro.html","loc":"crypto-intro.html"},{"title":"UFC Stats","text":"Background Ultimate Fighting Championship or UFC is based off a 10-9 scoring system similar to boxing and the fight goes to the judges unless there is a KO,TKO, DQ, or submission. Typically fights last 3 rounds of 5 minutes, with championship belts and top contender fights lasting 5 rounds of 5 minutes. For those interested, here is the current official MMA unified rules. Note that these have been changed and amended a few times since they were created in the early 2000s. How to Win As mentioned above there are a few different ways to end a UFC fight. Here we see both a pie chart and a time series graph of how fights end. I have removed all DQ and overturned results from the dataset and I have combined the split decision and majority decisions. I aggregate the data into a pie graph and we can see that about a third of the fights are unanimous decisions and another third are KO/TKOs. The other third are a combination of submissions and split or majority decisions. There are rarely any draws. However these percentages are not consistent. Like I mentioned previously there have been many iterations of rules and technique adaptations. The UFC looks vastly different now than in years past. To examine the changes, I create a rolling 5 year time series graph which again contains the various methods of how a fight can finish. The five year rolling method smooths the data and we see that the proportion of submissions and KO/TKOs have both decreased over the years with an increase in both the decision methods. This could hint that fighters are getting better at defense, leading to longer fighting times ultimately more decisions. Data I have built a database of UFC stats (starting from UFC 1 in November 12, 1993 and also incorporating promotions UFC bought like StrikeForce and WEC); in total, this results to a database of over 5000 fights and 3000 fighters. I am able to get fighters physical attributes like age and reach as well as certain fighting stats like knock down percentage. In addition, I have scraped a few betting websites for their lines, but unfortunately some of the earlier fights did not have betting lines. The betting data starts in 2012. I am working to release the scraping code and data on my github so others can create reproducible results and test new theories. When looking through this data it is important to note that the UFC is rapidly evolving and conclusions found today may not hold in a few years. There has been a lot of evolution within the UFC as in the beginning there was no time limits and only finishes, thus draws did not exist. Below is an example of a the fighting stats that I have collected. Here we see a recent fight between Molly McCann and Taila Santos. Similar to home and away in other sports, there is a red corner (Molly) and a blue corner (Talia) in which the red corner is usually the higher ranked fighter. Taking a glance at the specific statistics, we can see that the UFC records a multitude of stats that are broken down into two categories: striking stats and totals. The totals focus on miscellaneous stats like KD (knock downs), TD (take downs) as well as total strikes. The striking stats are broken down into the location on the body as well as the distance the strike was landed. In addition, strikes are classified as significant or not significant. We see Taila out-striking Molly and outclassing her in the take-down category, so it understanding that Taila won a unanimous decision, Features Going one step further when analyzing these stats, we can use feature engineering to break down these stats on a ratio basis think per minute stats. When Jorge Masvidal knocked out Ben Askren in 5 seconds with a flying knee, he only had 3 significant strikes, the first flying knee and two followup punches. Here is a clip of the fight. Looking only at the total number of significant strikes would penalize this career altering performance. This is an extreme example, but serves as an important reminder as to why changing stats to a common timeframe can be important.","tags":"UFC","url":"ufc-stats.html","loc":"ufc-stats.html"},{"title":"Basketball Introduction","text":"Introduction Given the name of the blog, its fair to say that I am a huge NBA fan. I have been watching the NBA for over 2 decades and saw first hand the rise of analytics in basketball. Statistical modelling has greatly impacted basketball strategy from drafting players all the way choosing what play to run. However, there is still a long way to go as there are many problems with the current state of basketball analytics. They often times only paint half the picture, and people just run with the result, rather than taking a moment to reflect on the short comings of these stats. Hot Hand Theory The best example I can think of where statisticians blindly followed the data and jumped to a conclusion is the hot-hand theory. The hot-hand theory is when a person experiences a successful outcome and then has a greater chance of success in future attempts. In basketball, it means there is positive autocorrelation with shot-making percentages. I am more likely to hit the next shot if I have already made a few, whether this is do to confidence, focus, or some other unexplainable reason. For years, statisticians and media members quoted a paper by Gilovich, Vallone, and Tversky which claimed that the hot hand did not exist. By looking at sequences of shooting, the authors concluded that the various 'hot hand' streaks were nothing more than random sequences of chance. Players and coaches were shocked at this conclusion. From a person who has been playing basketball at various levels since I was 5, I could not accept this conclusion either. I was doing mental gymnasts trying to reconcile what I was feeling and the data. This was a very controversial debate with people who played basketball on one side arguing against those who analyzed the data. Most of the basketball community had accepted the paper as gospel and the hot hand was renamed the hot hand fallacy. Finally, in 2014 the NBA got tracking data and the hot hand fallacy was debunked. A paper using tracking data was able to control for shot difficulty and disprove the hot hand fallacy. Players who are 'hot' shoot their next shot from further away and face a tougher defense, lowering the expected value of the shot. The conclusion is that players who are outperforming will continue to do so, conditional on the difficulty of their present shot. The Blog Similarly to the hot hand fallacy, our current statistics are have flaws when trying to describe the game, which leads to people often drawing wrong conclusions from the data. When people analyze a game, they often look at the box score which will often have around 15 categories, ranging from minutes to FG to points. I wish to tackle basketball analysis from first principles and examine the pros and cons of each box score statistic. I will do deep dives on new statistics that are important in describing the outcome of a game and show how these new metrics can help create optimal game play.","tags":"NBA","url":"basketball-introduction.html","loc":"basketball-introduction.html"},{"title":"About Me","text":"Introduction I am writing under this pseudonym in honor of my favorite athlete, someone who has inspired and motivated me throughout my life. He is arguably one of the best basketball ball players to ever play the game and yet he said his goal was to be remembered as an overachiever. RIP 2020-01-26. This blog is a place for me to reason out some of my thoughts in a more coherent manner and to create timestamps for these ideas. The main two topics will be my two biggest passions: finance and sports. On the finance side, I will write about interesting investments in the traditional markets and in the crypto space. Regarding sports, most of the posts will focus around sports analytics and sports betting. I would also like to apologize in advance for typos and bad grammar, my worst grades in college were in my humantities classes. Finance Traditional Markets I have been investing/trading for over a decade. In this blog, I want to focus on popular tailwinds and how they may benefit certain industries and players in the upcoming decade. In the sector space, I have a particular interest in tech (more specifically SaaS) sector and will be looking at these companies closely. In addition, I will also examine the global macro space from a long term investor standpoint. 1 Crypto Markets I have been around crypto since my college suite-mate introduced me to Litecoin mining in the early 2010's. Since then there have been two bull markets and two bear markets in this space, while stocks have just gone up. The pace of crypto in general moves faster than traditional markets and it also trades around the clock 24/7. I will be posting my thoughts about the future of crypto, especially as it relates to the various exchanges and DeFi space. Sports Sports analytics and sports betting are still in the nascent stages. The legal sports betting handle is a magnitude lower than the illegal sports betting handle, which leads to lower consolidated volume and higher vigs. Good clean data to do analytics is even harder to come by. Unlike in finance, where many fin-tech companies entire business model revolve around cleaning and selling data, when dealing with sports the data is hard to come by. For instance, the NBA actively changes their api to prevent scraping. The pipeline requires the ability to web-scrape, store, and clean the data before any analysis can be done. Nonetheless, I have built robust pipelines for the NBA 2 , NFL, and UFC and I will share interesting insights and tidbits on each sport. I will try to open source as much code as possible so others can play around with data and code. Closing Thoughts I have a statistics background and my day job is a quant so my posts will lean on the quantitative side. I can be reached at bobekryant33824@gmail.com if you have any questions or want to connect. I trade these products at a low latency for my day job. ↩ The NBA topics are decently technical. ↩","tags":"About Me","url":"about-me.html","loc":"about-me.html"}]};